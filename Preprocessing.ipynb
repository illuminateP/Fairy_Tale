{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2dff119-409d-4af0-8eba-211310e12918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753a328d-34db-4e8e-a368-b1efa7a6b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 폴더 기준 고유권수 : 1446\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "isbns = set()\n",
    "for dirpath, _, filenames in os.walk('./datasets/Training'):\n",
    "    for fn in filenames:\n",
    "        if fn.lower().endswith('.json'):\n",
    "            with open(os.path.join(dirpath, fn), encoding='utf-8-sig') as f:\n",
    "                d = json.load(f)\n",
    "            books = d if isinstance(d, list) else [d]\n",
    "            for book in books:\n",
    "                isbns.add(str(book.get('isbn','')).strip())\n",
    "print(f\"원본 폴더 기준 고유권수 : {len(isbns)}\")  # 원본 폴더 기준 고유권수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61647f68-687d-4b15-8f49-0ab2233ce7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 폴더 기준 고유권수 : 286\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "isbns = set()\n",
    "for dirpath, _, filenames in os.walk('./datasets/Validation'):\n",
    "    for fn in filenames:\n",
    "        if fn.lower().endswith('.json'):\n",
    "            with open(os.path.join(dirpath, fn), encoding='utf-8-sig') as f:\n",
    "                d = json.load(f)\n",
    "            books = d if isinstance(d, list) else [d]\n",
    "            for book in books:\n",
    "                isbns.add(str(book.get('isbn','')).strip())\n",
    "print(f\"원본 폴더 기준 고유권수 : {len(isbns)}\")  # 원본 폴더 기준 고유권수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0d7065-480e-4351-91db-b40b9e50cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec43cf3a-7249-46e5-9499-8016ce89cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.lower().endswith('.json'):\n",
    "            json_files.append(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7293c92d-b18c-4673-b7d1-7de5be8b668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn_title_set = set()\n",
    "isbn_meta_dict = {}\n",
    "\n",
    "for filepath in json_files:\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8-sig') as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):  # 혹시 리스트형이면\n",
    "            items = data\n",
    "        else:\n",
    "            items = [data]\n",
    "        for book in items:\n",
    "            isbn = str(book.get('isbn', '')).strip()\n",
    "            title = book.get('title', '').strip()\n",
    "            if isbn and title:\n",
    "                isbn_title_set.add((isbn, title))\n",
    "                # 메타만 남기기\n",
    "                meta = {\n",
    "                    \"isbn\": isbn,\n",
    "                    \"title\": title,\n",
    "                    \"author\": book.get('author', '').strip(),\n",
    "                    \"illustrator\": book.get('illustrator', '').strip(),\n",
    "                    \"readAge\": book.get('readAge', '').strip(),\n",
    "                    \"publishedYear\": book.get('publishedYear', ''),\n",
    "                    \"publisher\": book.get('publisher', '').strip(),\n",
    "                    \"classification\": book.get('classification', '').strip()\n",
    "                }\n",
    "                isbn_meta_dict[isbn] = meta\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {filepath}, {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df94db-d901-45e5-9df4-c249a678ba6a",
   "metadata": {},
   "source": [
    "### csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fd7d58-c08c-4cd9-b1f8-ba19c9349e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. isbn-title csv 저장 (1732 행)\n"
     ]
    }
   ],
   "source": [
    "# 1. isbn + title csv\n",
    "df_titles = pd.DataFrame(list(isbn_title_set), columns=['isbn', 'title']).sort_values('isbn')\n",
    "df_titles.to_csv(os.path.join(root_dir, 'isbn_title_only.csv'), index=False, encoding='utf-8-sig')\n",
    "print(f\"1. isbn-title csv 저장 ({len(df_titles)} 행)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad3819-086c-4d21-877e-216dba70fb2c",
   "metadata": {},
   "source": [
    "### json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f163bcf6-13eb-4888-af91-bd47275caeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. isbn별 메타 json 저장 (1732 파일)\n"
     ]
    }
   ],
   "source": [
    "# 2. isbn별 메타데이터 json 파일\n",
    "meta_dir = os.path.join(root_dir, 'isbn_meta')\n",
    "os.makedirs(meta_dir, exist_ok=True)\n",
    "for isbn, meta in isbn_meta_dict.items():\n",
    "    with open(os.path.join(meta_dir, f\"{isbn}.json\"), 'w', encoding='utf-8-sig') as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"2. isbn별 메타 json 저장 ({len(isbn_meta_dict)} 파일)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d4e89-4ef6-44de-9b3f-a2db2b061bcd",
   "metadata": {},
   "source": [
    "### meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d8a17f-08f8-4413-9ebc-c846b856af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. 전체 통합 메타 json 저장 (1732권)\n"
     ]
    }
   ],
   "source": [
    "# 3. 전체 통합 메타 json\n",
    "all_meta_path = os.path.join(root_dir, 'all_isbn_meta.json')\n",
    "with open(all_meta_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump(list(isbn_meta_dict.values()), f, ensure_ascii=False, indent=2)\n",
    "print(f\"3. 전체 통합 메타 json 저장 ({len(isbn_meta_dict)}권)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516311bc-e873-4173-9a08-dfb48861c291",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e1e5b-3c89-46bf-942b-96bdc8283aff",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66ea76cb-32ff-488f-8473-1c84ad18e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 isbn 고유권수: 1446\n",
      "명시적 질문 1개 이상 있는 책 권수: 1443\n",
      "명시적 질문 0개인 책 권수: 3\n",
      "[Sublabel 없음] 9791159420214\n",
      "[Sublabel 없음] 9791186922972\n",
      "[Sublabel 없음] 9788961914314\n",
      "Sublabel 복사: 1440개 성공, 3개 실패\n",
      "[최종] 전체 고유 isbn: 1446\n",
      "[최종] 명시적 질문 있는 책: 1443\n",
      "[최종] 명시적 질문 없는 책: 3\n",
      "모든 전처리/저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "root_dir = './datasets/Training'\n",
    "sublabel_dir = './datasets/Sublabel'\n",
    "converted_root = './converted/training'\n",
    "converted_sublabel = './converted/Sublabel'\n",
    "json_dir = os.path.join(converted_root, 'json')\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "os.makedirs(converted_sublabel, exist_ok=True)\n",
    "\n",
    "# --- 1. isbn별 메타/질문 집계 ---\n",
    "isbn_info = {}\n",
    "isbn_explicit = defaultdict(bool)\n",
    "\n",
    "json_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.lower().endswith('.json'):\n",
    "            json_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "for src_path in json_files:\n",
    "    with open(src_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    books = data if isinstance(data, list) else [data]\n",
    "    for book in books:\n",
    "        isbn = str(book.get('isbn', '')).strip()\n",
    "        title = book.get('title', '').strip()\n",
    "        if not isbn:\n",
    "            continue\n",
    "        # 메타 정보 최초 1회만 기록\n",
    "        if isbn not in isbn_info:\n",
    "            meta = {\"isbn\": isbn, \"title\": title}\n",
    "            for key in [\"author\", \"illustrator\", \"readAge\", \"publishedYear\", \"publisher\", \"classification\"]:\n",
    "                if key in book and book[key]:\n",
    "                    meta[key] = str(book[key]).strip()\n",
    "            isbn_info[isbn] = meta\n",
    "        # 명시적 질문 있는지 체크\n",
    "        for para in book.get('paragraphInfo', []):\n",
    "            for qa in para.get('queAnsPairInfo', []):\n",
    "                if '명시적' in qa.get('ansType', ''):\n",
    "                    isbn_explicit[isbn] = True\n",
    "\n",
    "isbn_all_set = set(isbn_info.keys())\n",
    "isbn_explicit_set = set(isbn for isbn, flag in isbn_explicit.items() if flag)\n",
    "isbn_removed_set = isbn_all_set - isbn_explicit_set\n",
    "\n",
    "print(f\"전체 isbn 고유권수: {len(isbn_all_set)}\")\n",
    "print(f\"명시적 질문 1개 이상 있는 책 권수: {len(isbn_explicit_set)}\")\n",
    "print(f\"명시적 질문 0개인 책 권수: {len(isbn_removed_set)}\")\n",
    "\n",
    "# --- 2. 변환 json/meta ---\n",
    "meta_list = []\n",
    "meta_dict = {}\n",
    "\n",
    "for src_path in json_files:\n",
    "    with open(src_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    books = data if isinstance(data, list) else [data]\n",
    "    new_books = []\n",
    "    for book in books:\n",
    "        isbn = str(book.get('isbn', '')).strip()\n",
    "        if isbn not in isbn_explicit_set:\n",
    "            continue\n",
    "        # 명시적 질문만 남긴다\n",
    "        book_copy = dict(book)\n",
    "        new_paragraphs = []\n",
    "        for para in book.get('paragraphInfo', []):\n",
    "            new_para = dict(para)\n",
    "            new_qapairs = []\n",
    "            for qa in para.get('queAnsPairInfo', []):\n",
    "                if '명시적' in qa.get('ansType', ''):\n",
    "                    new_qapairs.append(qa)\n",
    "            new_para['queAnsPairInfo'] = new_qapairs\n",
    "            new_para['queAnsPairInfoCount'] = len(new_qapairs)\n",
    "            new_paragraphs.append(new_para)\n",
    "        book_copy['paragraphInfo'] = new_paragraphs\n",
    "        new_books.append(book_copy)\n",
    "    if new_books:\n",
    "        rel_path = os.path.relpath(src_path, root_dir)\n",
    "        save_path = os.path.join(converted_root, rel_path)\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, 'w', encoding='utf-8-sig') as f:\n",
    "            if isinstance(data, list):\n",
    "                json.dump(new_books, f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                json.dump(new_books[0], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "for isbn in sorted(isbn_explicit_set):\n",
    "    meta_dict[isbn] = isbn_info[isbn]\n",
    "    meta_list.append(isbn_info[isbn])\n",
    "\n",
    "# --- 3. Sublabel 복사(하위폴더 전체에서 isbn에 맞는 파일 찾기) ---\n",
    "def find_sublabel_file(sublabel_dir, isbn):\n",
    "    for dirpath, _, filenames in os.walk(sublabel_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(f\"{isbn}.json\"):\n",
    "                return os.path.join(dirpath, filename)\n",
    "    return None\n",
    "\n",
    "copy_ok, copy_fail = 0, 0\n",
    "for isbn in isbn_explicit_set:\n",
    "    src_full = find_sublabel_file(sublabel_dir, isbn)\n",
    "    dst_full = os.path.join(converted_sublabel, f\"{isbn}.json\")\n",
    "    if src_full and os.path.exists(src_full):\n",
    "        shutil.copy2(src_full, dst_full)\n",
    "        copy_ok += 1\n",
    "    else:\n",
    "        print(f\"[Sublabel 없음] {isbn}\")\n",
    "        copy_fail += 1\n",
    "print(f\"Sublabel 복사: {copy_ok}개 성공, {copy_fail}개 실패\")\n",
    "\n",
    "# --- 4. 결과 저장 ---\n",
    "meta_simple_path = os.path.join(json_dir, 'book_titles_by_isbn.json')\n",
    "with open(meta_simple_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump([{\"isbn\": m[\"isbn\"], \"title\": m[\"title\"]} for m in meta_list], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "meta_all_path = os.path.join(json_dir, 'book_meta_all.json')\n",
    "with open(meta_all_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump(meta_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "meta_books_dir = os.path.join(json_dir, 'books_by_isbn')\n",
    "os.makedirs(meta_books_dir, exist_ok=True)\n",
    "for isbn, meta in meta_dict.items():\n",
    "    meta_path = os.path.join(meta_books_dir, f'{isbn}.json')\n",
    "    with open(meta_path, 'w', encoding='utf-8-sig') as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "removed_path = os.path.join(json_dir, 'removed_books.csv')\n",
    "pd.DataFrame([isbn_info[i] for i in sorted(isbn_removed_set)]).to_csv(removed_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"[최종] 전체 고유 isbn: {len(isbn_all_set)}\")\n",
    "print(f\"[최종] 명시적 질문 있는 책: {len(isbn_explicit_set)}\")\n",
    "print(f\"[최종] 명시적 질문 없는 책: {len(isbn_removed_set)}\")\n",
    "print(f\"모든 전처리/저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcd0f4-dec3-4ca4-a4a5-6001c9df4e66",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "463647ac-7950-4f0a-90c6-857111e232fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 isbn 고유권수: 286\n",
      "명시적 질문 1개 이상 있는 책 권수: 284\n",
      "명시적 질문 0개인 책 권수: 2\n",
      "[Sublabel 없음] 9791159420160\n",
      "[Sublabel 없음] 9791159420054\n",
      "[Sublabel 없음] 9791128208904\n",
      "Sublabel 복사: 281개 성공, 3개 실패\n",
      "[최종] 전체 고유 isbn: 286\n",
      "[최종] 명시적 질문 있는 책: 284\n",
      "[최종] 명시적 질문 없는 책: 2\n",
      "모든 전처리/저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "root_dir = './datasets/Validation'\n",
    "sublabel_dir = './datasets/Sublabel'\n",
    "converted_root = './converted/validation'\n",
    "converted_sublabel = './converted/Sublabel'\n",
    "json_dir = os.path.join(converted_root, 'json')\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "os.makedirs(converted_sublabel, exist_ok=True)\n",
    "\n",
    "# --- 1. isbn별 메타/질문 집계 ---\n",
    "isbn_info = {}\n",
    "isbn_explicit = defaultdict(bool)\n",
    "\n",
    "json_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.lower().endswith('.json'):\n",
    "            json_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "for src_path in json_files:\n",
    "    with open(src_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    books = data if isinstance(data, list) else [data]\n",
    "    for book in books:\n",
    "        isbn = str(book.get('isbn', '')).strip()\n",
    "        title = book.get('title', '').strip()\n",
    "        if not isbn:\n",
    "            continue\n",
    "        # 메타 정보 최초 1회만 기록\n",
    "        if isbn not in isbn_info:\n",
    "            meta = {\"isbn\": isbn, \"title\": title}\n",
    "            for key in [\"author\", \"illustrator\", \"readAge\", \"publishedYear\", \"publisher\", \"classification\"]:\n",
    "                if key in book and book[key]:\n",
    "                    meta[key] = str(book[key]).strip()\n",
    "            isbn_info[isbn] = meta\n",
    "        # 명시적 질문 있는지 체크\n",
    "        for para in book.get('paragraphInfo', []):\n",
    "            for qa in para.get('queAnsPairInfo', []):\n",
    "                if '명시적' in qa.get('ansType', ''):\n",
    "                    isbn_explicit[isbn] = True\n",
    "\n",
    "isbn_all_set = set(isbn_info.keys())\n",
    "isbn_explicit_set = set(isbn for isbn, flag in isbn_explicit.items() if flag)\n",
    "isbn_removed_set = isbn_all_set - isbn_explicit_set\n",
    "\n",
    "print(f\"전체 isbn 고유권수: {len(isbn_all_set)}\")\n",
    "print(f\"명시적 질문 1개 이상 있는 책 권수: {len(isbn_explicit_set)}\")\n",
    "print(f\"명시적 질문 0개인 책 권수: {len(isbn_removed_set)}\")\n",
    "\n",
    "# --- 2. 변환 json/meta ---\n",
    "meta_list = []\n",
    "meta_dict = {}\n",
    "\n",
    "for src_path in json_files:\n",
    "    with open(src_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    books = data if isinstance(data, list) else [data]\n",
    "    new_books = []\n",
    "    for book in books:\n",
    "        isbn = str(book.get('isbn', '')).strip()\n",
    "        if isbn not in isbn_explicit_set:\n",
    "            continue\n",
    "        # 명시적 질문만 남긴다\n",
    "        book_copy = dict(book)\n",
    "        new_paragraphs = []\n",
    "        for para in book.get('paragraphInfo', []):\n",
    "            new_para = dict(para)\n",
    "            new_qapairs = []\n",
    "            for qa in para.get('queAnsPairInfo', []):\n",
    "                if '명시적' in qa.get('ansType', ''):\n",
    "                    new_qapairs.append(qa)\n",
    "            new_para['queAnsPairInfo'] = new_qapairs\n",
    "            new_para['queAnsPairInfoCount'] = len(new_qapairs)\n",
    "            new_paragraphs.append(new_para)\n",
    "        book_copy['paragraphInfo'] = new_paragraphs\n",
    "        new_books.append(book_copy)\n",
    "    if new_books:\n",
    "        rel_path = os.path.relpath(src_path, root_dir)\n",
    "        save_path = os.path.join(converted_root, rel_path)\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, 'w', encoding='utf-8-sig') as f:\n",
    "            if isinstance(data, list):\n",
    "                json.dump(new_books, f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                json.dump(new_books[0], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "for isbn in sorted(isbn_explicit_set):\n",
    "    meta_dict[isbn] = isbn_info[isbn]\n",
    "    meta_list.append(isbn_info[isbn])\n",
    "\n",
    "# --- 3. Sublabel 복사(하위폴더 전체에서 isbn에 맞는 파일 찾기) ---\n",
    "def find_sublabel_file(sublabel_dir, isbn):\n",
    "    for dirpath, _, filenames in os.walk(sublabel_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(f\"{isbn}.json\"):\n",
    "                return os.path.join(dirpath, filename)\n",
    "    return None\n",
    "\n",
    "copy_ok, copy_fail = 0, 0\n",
    "for isbn in isbn_explicit_set:\n",
    "    src_full = find_sublabel_file(sublabel_dir, isbn)\n",
    "    dst_full = os.path.join(converted_sublabel, f\"{isbn}.json\")\n",
    "    if src_full and os.path.exists(src_full):\n",
    "        shutil.copy2(src_full, dst_full)\n",
    "        copy_ok += 1\n",
    "    else:\n",
    "        print(f\"[Sublabel 없음] {isbn}\")\n",
    "        copy_fail += 1\n",
    "print(f\"Sublabel 복사: {copy_ok}개 성공, {copy_fail}개 실패\")\n",
    "\n",
    "# --- 4. 결과 저장 ---\n",
    "meta_simple_path = os.path.join(json_dir, 'book_titles_by_isbn.json')\n",
    "with open(meta_simple_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump([{\"isbn\": m[\"isbn\"], \"title\": m[\"title\"]} for m in meta_list], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "meta_all_path = os.path.join(json_dir, 'book_meta_all.json')\n",
    "with open(meta_all_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump(meta_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "meta_books_dir = os.path.join(json_dir, 'books_by_isbn')\n",
    "os.makedirs(meta_books_dir, exist_ok=True)\n",
    "for isbn, meta in meta_dict.items():\n",
    "    meta_path = os.path.join(meta_books_dir, f'{isbn}.json')\n",
    "    with open(meta_path, 'w', encoding='utf-8-sig') as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "removed_path = os.path.join(json_dir, 'removed_books.csv')\n",
    "pd.DataFrame([isbn_info[i] for i in sorted(isbn_removed_set)]).to_csv(removed_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"[최종] 전체 고유 isbn: {len(isbn_all_set)}\")\n",
    "print(f\"[최종] 명시적 질문 있는 책: {len(isbn_explicit_set)}\")\n",
    "print(f\"[최종] 명시적 질문 없는 책: {len(isbn_removed_set)}\")\n",
    "print(f\"모든 전처리/저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d08bf351-2ade-4468-bb72-b8eb751f5191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 전체 JSON 파일 수: 1721\n"
     ]
    }
   ],
   "source": [
    "# 전체 JSON 파일 개수 세기\n",
    "json_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"./converted/Sublabel\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            json_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"📂 전체 JSON 파일 수: {len(json_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64284079-a4f2-4654-8228-ab5907f2ee8d",
   "metadata": {},
   "source": [
    "### Unpacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f878010-1550-4ebd-a5a4-14e06ce399ce",
   "metadata": {},
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "base_dir = './converted/sublabel'\n",
    "\n",
    "# 이동된 파일 수를 세기 위한 변수\n",
    "moved_files_count = 0\n",
    "\n",
    "# 01.원천데이터 폴더가 존재하는지 확인\n",
    "if not os.path.isdir(base_dir):\n",
    "    print(f\"오류: 지정된 경로 '{base_dir}'가 존재하지 않거나 폴더가 아닙니다.\")\n",
    "else:\n",
    "    print(f\"'{base_dir}' 폴더에서 .json 파일 이동을 시작합니다.\")\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir, topdown=False):\n",
    "        if root == base_dir:\n",
    "            continue\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.json'):\n",
    "                source_path = os.path.join(root, file_name)\n",
    "                destination_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "                # 파일명 충돌 방지: 이미 대상 폴더에 동일한 파일명이 존재할 경우 처리\n",
    "                if os.path.exists(destination_path):\n",
    "                    # 충돌 해결 전략: (3) 이름 변경\n",
    "                    base, ext = os.path.splitext(file_name)\n",
    "                    counter = 1\n",
    "                    new_file_name = f\"{base}_{counter}{ext}\"\n",
    "                    while os.path.exists(os.path.join(base_dir, new_file_name)):\n",
    "                        counter += 1\n",
    "                        new_file_name = f\"{base}_{counter}{ext}\"\n",
    "                    \n",
    "                    destination_path = os.path.join(base_dir, new_file_name)\n",
    "                    print(f\"파일명 충돌: '{file_name}' -> '{new_file_name}'으로 변경하여 이동합니다.\")\n",
    "                \n",
    "                try:\n",
    "                    shutil.move(source_path, destination_path)\n",
    "                    print(f\"'{source_path}' -> '{destination_path}' (이동 완료)\")\n",
    "                    moved_files_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"'{source_path}' 이동 중 오류 발생: {e}\")\n",
    "        \n",
    "        # 파일 이동 후, 현재 하위 디렉토리(root)가 비어있으면 삭제 \n",
    "        try:\n",
    "            if not os.listdir(root):\n",
    "                os.rmdir(root)\n",
    "                print(f\"빈 디렉토리 삭제: '{root}'\")\n",
    "        except OSError as e:\n",
    "            print(f\"빈 디렉토리 '{root}' 삭제 중 오류 발생 (아마도 비어있지 않거나 권한 문제): {e}\")\n",
    "\n",
    "    print(f\"\\n모든 .json 파일 이동이 완료되었습니다. 총 {moved_files_count}개의 파일이 이동되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342b87d-cdfb-48e4-883c-d1e7bd0786e9",
   "metadata": {},
   "source": [
    "### Lookup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76a6661a-6295-4618-9aff-247f4777e5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./converted\\training' 에서 .json 파일 목록을 수집 중...\n",
      "'./converted\\training' 에서 총 1443개의 고유 .json 파일명 수집 완료.\n",
      "'./converted\\validation' 에서 .json 파일 목록을 수집 중...\n",
      "'./converted\\validation' 에서 추가로 284개의 고유 .json 파일명 수집 완료.\n",
      "\n",
      "총 (training + validation) 고유 .json 파일 개수: 1727개\n",
      "\n",
      "'./converted\\sublabel' 에서 .json 파일 목록을 수집 중...\n",
      "'./converted\\sublabel' 에서 총 2342개의 고유 .json 파일명 수집 완료.\n",
      "\n",
      "--- 비교 결과 ---\n",
      "Training/Validation 총 고유 파일 개수: 1727개\n",
      "Sublabel 총 고유 파일 개수: 2342개\n",
      "\n",
      "❌ Sublabel에 없는 Training/Validation 파일 개수: 4개\n",
      "\n",
      "Sublabel에 없는 파일 목록:\n",
      "- 03_02T_02S_9788961914314.json\n",
      "- 03_03T_03S_9791128208904.json\n",
      "- 03_03T_03S_9791159420054_.json\n",
      "- 03_03T_03S_9791159420160_.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_sublabel_completeness(converted_base_dir):\n",
    "    \"\"\"\n",
    "    training, validation 경로의 .json 파일명들을 sublabel 경로의 .json 파일명과 비교하여\n",
    "    누락된 목록과 개수를 출력합니다.\n",
    "\n",
    "    Args:\n",
    "        converted_base_dir (str): 'converted' 폴더의 절대 또는 상대 경로.\n",
    "                                  예: './converted' 또는 'C:/Users/user/project/converted'\n",
    "    \"\"\"\n",
    "    training_dir = os.path.join(converted_base_dir, 'training')\n",
    "    validation_dir = os.path.join(converted_base_dir, 'validation')\n",
    "    sublabel_dir = os.path.join(converted_base_dir, 'sublabel')\n",
    "\n",
    "    # 1. training 및 validation 경로의 모든 JSON 파일명 수집 (중복 제거)\n",
    "    all_source_filenames = set()\n",
    "\n",
    "    # training 디렉토리 탐색\n",
    "    print(f\"'{training_dir}' 에서 .json 파일 목록을 수집 중...\")\n",
    "    if not os.path.isdir(training_dir):\n",
    "        print(f\"경고: '{training_dir}' 경로가 존재하지 않습니다.\")\n",
    "    else:\n",
    "        for root, _, files in os.walk(training_dir):\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.json'):\n",
    "                    all_source_filenames.add(file_name)\n",
    "        print(f\"'{training_dir}' 에서 총 {len(all_source_filenames)}개의 고유 .json 파일명 수집 완료.\")\n",
    "\n",
    "\n",
    "    # validation 디렉토리 탐색 (기존 all_source_filenames에 추가)\n",
    "    print(f\"'{validation_dir}' 에서 .json 파일 목록을 수집 중...\")\n",
    "    if not os.path.isdir(validation_dir):\n",
    "        print(f\"경고: '{validation_dir}' 경로가 존재하지 않습니다.\")\n",
    "    else:\n",
    "        current_source_count = len(all_source_filenames)\n",
    "        for root, _, files in os.walk(validation_dir):\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.json'):\n",
    "                    all_source_filenames.add(file_name)\n",
    "        print(f\"'{validation_dir}' 에서 추가로 {len(all_source_filenames) - current_source_count}개의 고유 .json 파일명 수집 완료.\")\n",
    "\n",
    "    total_source_count = len(all_source_filenames)\n",
    "    print(f\"\\n총 (training + validation) 고유 .json 파일 개수: {total_source_count}개\")\n",
    "\n",
    "    # 2. sublabel 경로의 모든 JSON 파일명 수집\n",
    "    all_sublabel_filenames = set()\n",
    "    print(f\"\\n'{sublabel_dir}' 에서 .json 파일 목록을 수집 중...\")\n",
    "    if not os.path.isdir(sublabel_dir):\n",
    "        print(f\"오류: '{sublabel_dir}' 경로가 존재하지 않습니다. 이 폴더가 없으면 비교할 수 없습니다.\")\n",
    "        return # sublabel 폴더가 없으면 함수 종료\n",
    "\n",
    "    for root, _, files in os.walk(sublabel_dir):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.json'):\n",
    "                all_sublabel_filenames.add(file_name)\n",
    "\n",
    "    total_sublabel_count = len(all_sublabel_filenames)\n",
    "    print(f\"'{sublabel_dir}' 에서 총 {total_sublabel_count}개의 고유 .json 파일명 수집 완료.\")\n",
    "\n",
    "    # 3. 비교 및 결과 출력\n",
    "    # training/validation에는 있지만 sublabel에는 없는 파일\n",
    "    missing_in_sublabel = all_source_filenames - all_sublabel_filenames\n",
    "    \n",
    "    print(\"\\n--- 비교 결과 ---\")\n",
    "    print(f\"Training/Validation 총 고유 파일 개수: {total_source_count}개\")\n",
    "    print(f\"Sublabel 총 고유 파일 개수: {total_sublabel_count}개\")\n",
    "\n",
    "    if not missing_in_sublabel:\n",
    "        print(\"\\n✅ 모든 Training/Validation 파일이 Sublabel에 존재합니다!\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Sublabel에 없는 Training/Validation 파일 개수: {len(missing_in_sublabel)}개\")\n",
    "        print(\"\\nSublabel에 없는 파일 목록:\")\n",
    "        # 정렬하여 출력하면 보기 좋습니다.\n",
    "        for filename in sorted(list(missing_in_sublabel)):\n",
    "            print(f\"- {filename}\")\n",
    "\n",
    "# --- 함수 호출 ---\n",
    "# 여기에 'converted' 폴더의 실제 경로를 입력하세요.\n",
    "# 예: converted_base_directory = 'C:/Users/사용자명/내프로젝트/converted'\n",
    "# 또는 현재 주피터 노트북 파일이 'converted' 폴더와 같은 레벨에 있다면:\n",
    "converted_base_directory = './converted' \n",
    "\n",
    "check_sublabel_completeness(converted_base_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e9dde-5060-4b67-8ec3-ff3bb3b2ae1c",
   "metadata": {},
   "source": [
    "### Check token size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ead606-bb94-42a4-8d28-cb5383039353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sys, re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE = Path(\"./converted/formatted\")\n",
    "SPLITS = [\"train\", \"val\"]\n",
    "\n",
    "def load_json(fp):\n",
    "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def fail(msg, errors, fp):\n",
    "    errors.append(f\"{fp} :: {msg}\")\n",
    "\n",
    "def validate_file(fp):\n",
    "    errors=[]\n",
    "    try:\n",
    "        js = load_json(fp)\n",
    "    except Exception as e:\n",
    "        return [f\"{fp} :: JSON load error – {e}\"]\n",
    "\n",
    "    if \"data\"   not in js: fail(\"'data' key missing\", errors, fp)\n",
    "    if \"version\" not in js: fail(\"'version' key missing\", errors, fp)\n",
    "\n",
    "    for d_i, data in enumerate(js.get(\"data\", [])):\n",
    "        if \"title\" not in data: fail(f\"[data[{d_i}]] 'title' missing\", errors, fp)\n",
    "        for p_i, para in enumerate(data.get(\"paragraphs\", [])):\n",
    "            context = para.get(\"context\")\n",
    "            if context is None: fail(f\"[{d_i}][{p_i}] context missing\", errors, fp)\n",
    "            for q_i, qa in enumerate(para.get(\"qas\", [])):\n",
    "                qid = qa.get(\"id\", \"<no-id>\")\n",
    "                if \"question\" not in qa:  fail(f\"{qid} question missing\", errors, fp)\n",
    "                if \"is_impossible\" not in qa:\n",
    "                    fail(f\"{qid} is_impossible missing\", errors, fp)\n",
    "                    continue\n",
    "                imps = qa[\"is_impossible\"]\n",
    "\n",
    "                ans_list = qa.get(\"answers\", [])\n",
    "                if imps and ans_list:\n",
    "                    fail(f\"{qid} marked impossible but answers provided\", errors, fp)\n",
    "                if not imps and not ans_list:\n",
    "                    fail(f\"{qid} possible but answers empty\", errors, fp)\n",
    "\n",
    "                # answer-context 정합성\n",
    "                for a_i, ans in enumerate(ans_list):\n",
    "                    text = ans.get(\"text\")\n",
    "                    pos  = ans.get(\"answer_start\")\n",
    "                    if text is None or pos is None:\n",
    "                        fail(f\"{qid} answer[{a_i}] missing field\", errors, fp)\n",
    "                        continue\n",
    "                    if context is not None and context[pos:pos+len(text)] != text:\n",
    "                        snippet = context[pos:pos+len(text)]\n",
    "                        fail(f\"{qid} answer mismatch (ctx:'{snippet}' vs ans:'{text}')\", errors, fp)\n",
    "    return errors\n",
    "\n",
    "def validate_split(split):\n",
    "    print(f\"\\n🔍 VALIDATE {split.upper()}\")\n",
    "    split_dir = BASE / split\n",
    "    issues=[]\n",
    "    for fp in tqdm(list(split_dir.glob(\"*.json\"))):\n",
    "        issues.extend(validate_file(fp))\n",
    "    if issues:\n",
    "        print(f\"❌ {len(issues)} issue(s) found in {split} files\")\n",
    "        for msg in issues[:20]:   # 처음 20개만 미리보기\n",
    "            print(\"   •\", msg)\n",
    "    else:\n",
    "        print(\"✅ all {split} files are valid\")\n",
    "\n",
    "for s in SPLITS:\n",
    "    validate_split(s)\n",
    "\n",
    "print(\"\\n🟢 검증 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ff4f6-64db-4ace-b631-c07fe18eb6a2",
   "metadata": {},
   "source": [
    "### formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bfc5a6a-fc6b-49a0-bd7f-5f2b83f4cb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄  loading …\n",
      "  train label 1443 | val label 284 | sub 1725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8feee4a60fe42ee86149ccdb5c91962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAIN:   0%|          | 0/1443 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ train.json — books: 1429  skipped_isbn: 1\n",
      "🔹 총 QA 수: 2521 | 미포함 QA 수: 85\n",
      "   ↪️  말뭉치 누락 ISBN 예시: ['9788961914314']\n",
      "   ↪️  정답 미포함 QA 예시:\n",
      "      • [9791128212765] Q: 무서운 뱀인줄 알고 어떻게 하자고 했나요? / A: 도망가자.\n",
      "      • [9791128216725] Q: 여우가 우주선을 타고 있는 걸 본 친구들은 뭐라고 했나요? / A: 우리도 태워 줘.\n",
      "      • [9791128216985] Q: 도토리 달을 먹은 다람쥐는 어떻게 하나요? / A: 다람쥐가 웃어요. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf793afca4042be9afd455a7af49c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VAL:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ val.json — books: 276  skipped_isbn: 1\n",
      "🔹 총 QA 수: 512 | 미포함 QA 수: 22\n",
      "   ↪️  말뭉치 누락 ISBN 예시: ['9791128208904']\n",
      "   ↪️  정답 미포함 QA 예시:\n",
      "      • [9791165432423] Q: 친구와 함께 그네를 탔을 때 어떤 기분을 느낄까요? / A: 신난다.\n",
      "      • [9791165434724] Q: 아이의 배변을 도와주려고 하는 이는 누구인가요? / A: 야옹이\n",
      "      • [9791165435950] Q: 책은 혼자 보는 것보다 어떻게 보는 것이 더 재미있을까요? / A: 친구와 함께 보면 더 재미나요.\n"
     ]
    }
   ],
   "source": [
    "# ╔═══════════════╗\n",
    "# ║  🟢  Cell 1   ║  KorQuAD 형식 말뭉치 생성 스크립트\n",
    "# ╚═══════════════╝\n",
    "import json, re, hashlib\n",
    "from pathlib import Path\n",
    "from typing import Union, Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ───────── 경로 설정 ─────────\n",
    "ROOT      = Path('./converted')\n",
    "TRAIN_LB  = ROOT/'training'  /'02.라벨링데이터'\n",
    "VAL_LB    = ROOT/'validation'/'02.라벨링데이터'\n",
    "SUB_DIR   = ROOT/'sublabel'\n",
    "OUT_DIR   = ROOT/'formatted'; OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ───────── ISBN 추출 ─────────\n",
    "ISBN_RE = re.compile(r'(\\d{9,13}X?)_?(?=\\.json$)')\n",
    "def isbn_from_name(src: Union[str, Path]) -> str:\n",
    "    name = src.name if hasattr(src, 'name') else str(src)\n",
    "    m = ISBN_RE.search(name)\n",
    "    if not m:\n",
    "        raise ValueError(f'ISBN not found: {name}')\n",
    "    return m.group(1)\n",
    "\n",
    "# ───────── 데이터 로딩 ─────────\n",
    "def load_dir(p: Path) -> Dict[str, dict]:\n",
    "    return {isbn_from_name(f): json.loads(f.read_text('utf-8-sig')) for f in p.glob('*.json')}\n",
    "\n",
    "def load_sublabels(p: Path) -> Dict[str, str]:\n",
    "    tmp: Dict[str, List[str]] = {}\n",
    "    for f in p.glob('*.json'):\n",
    "        isbn = isbn_from_name(f)\n",
    "        txt = json.loads(f.read_text('utf-8-sig'))['text']\n",
    "        tmp.setdefault(isbn, []).append(txt)\n",
    "    return {k: '\\n\\n'.join(v) for k, v in tmp.items()}\n",
    "\n",
    "print(\"📄  loading …\")\n",
    "train_lbl = load_dir(TRAIN_LB)\n",
    "val_lbl   = load_dir(VAL_LB)\n",
    "sub_txt   = load_sublabels(SUB_DIR)\n",
    "print(f\"  train label {len(train_lbl)} | val label {len(val_lbl)} | sub {len(sub_txt)}\")\n",
    "\n",
    "# ───────── KorQuAD entry 생성 ─────────\n",
    "def entry(isbn: str, lbl: dict, context: str) -> tuple[dict, list]:\n",
    "    qas, missed = [], []\n",
    "    for qa in lbl[\"paragraphInfo\"][0][\"queAnsPairInfo\"]:\n",
    "        qid = f'{isbn}-{hashlib.md5(qa[\"question\"].encode()).hexdigest()[:8]}'\n",
    "        ans = qa[\"ansM1\"]\n",
    "        if not ans: continue\n",
    "        start = context.find(ans)\n",
    "        if start == -1:\n",
    "            missed.append((qa[\"question\"], ans))\n",
    "            continue\n",
    "        qas.append({\n",
    "            \"id\": qid,\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"answers\": [{\"text\": ans, \"answer_start\": start}]\n",
    "        })\n",
    "    return {\n",
    "        \"title\": lbl[\"title\"],\n",
    "        \"paragraphs\": [{\n",
    "            \"context\": context,\n",
    "            \"qas\": qas\n",
    "        }]\n",
    "    }, missed\n",
    "\n",
    "# ───────── Split 생성 및 저장 ─────────\n",
    "def make_split(lbl_pool: Dict[str, dict], split: str):\n",
    "    data, skip_isbn = [], []\n",
    "    total_qas, missed_qas_total = 0, 0\n",
    "    missed_detail = []\n",
    "\n",
    "    for isbn, lbl in tqdm(lbl_pool.items(), desc=f\"{split.upper()}\"):\n",
    "        context = sub_txt.get(isbn)\n",
    "        if not context:\n",
    "            skip_isbn.append(isbn)\n",
    "            continue\n",
    "\n",
    "        record, missed = entry(isbn, lbl, context)\n",
    "        total_qas += sum(1 for _ in lbl[\"paragraphInfo\"][0][\"queAnsPairInfo\"])\n",
    "        missed_qas_total += len(missed)\n",
    "        if record[\"paragraphs\"][0][\"qas\"]:\n",
    "            data.append(record)\n",
    "        if missed:\n",
    "            missed_detail.append((isbn, missed))\n",
    "\n",
    "    out_path = OUT_DIR / f\"{split}.json\"\n",
    "    json.dump({\"version\": \"v1.0\", \"data\": data}, out_path.open('w', encoding='utf-8'), ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n✅ {out_path.name} — books: {len(data)}  skipped_isbn: {len(skip_isbn)}\")\n",
    "    print(f\"🔹 총 QA 수: {total_qas} | 미포함 QA 수: {missed_qas_total}\")\n",
    "    if skip_isbn:\n",
    "        print(f\"   ↪️  말뭉치 누락 ISBN 예시: {skip_isbn[:5]}\")\n",
    "    if missed_detail:\n",
    "        print(\"   ↪️  정답 미포함 QA 예시:\")\n",
    "        for isbn, qa_list in missed_detail[:3]:\n",
    "            for q, a in qa_list[:2]:\n",
    "                print(f\"      • [{isbn}] Q: {q} / A: {a}\")\n",
    "\n",
    "# ───────── 실행 ─────────\n",
    "make_split(train_lbl, 'train')\n",
    "make_split(val_lbl,   'val')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c43529-b2a5-4063-880b-121a8eab4078",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547a1c76-d9e9-4953-b1f3-b114ed5915cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📜 train.json\n",
      "  articles        : 1429\n",
      "  paragraphs      : 1429\n",
      "  QAs             : 2436\n",
      "  ├── empty answer      : 0\n",
      "  ├── start OOB         : 0\n",
      "  └── substring mismatch: 0\n",
      "  context length  : max 185806, avg 3079.5\n",
      "  answer length   : max 63, avg 5.5\n",
      "\n",
      "📜 val.json\n",
      "  articles        : 276\n",
      "  paragraphs      : 276\n",
      "  QAs             : 490\n",
      "  ├── empty answer      : 0\n",
      "  ├── start OOB         : 0\n",
      "  └── substring mismatch: 0\n",
      "  context length  : max 129815, avg 2454.8\n",
      "  answer length   : max 36, avg 5.5\n"
     ]
    }
   ],
   "source": [
    "# ╔═══════════════╗\n",
    "# ║  🟢  Cell 2   ║  상세 검증 스크립트\n",
    "# ╚═══════════════╝\n",
    "\n",
    "import json, textwrap, statistics\n",
    "from pathlib import Path\n",
    "\n",
    "def validate(split: str, root: Path = Path(\"./converted/formatted\")):\n",
    "    path = root / f\"{split}.json\"\n",
    "    if not path.exists():\n",
    "        print(f\"{split}: 파일이 없습니다.\")\n",
    "        return\n",
    "    blob = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    art_cnt = par_cnt = qa_cnt = 0\n",
    "    empty_ans = out_of_bounds = mismatch = 0\n",
    "    bad_samples = []\n",
    "\n",
    "    ctx_lengths = []\n",
    "    ans_lengths = []\n",
    "\n",
    "    for art in blob[\"data\"]:\n",
    "        art_cnt += 1\n",
    "        for par in art[\"paragraphs\"]:\n",
    "            par_cnt += 1\n",
    "            ctx = par[\"context\"]\n",
    "            ctx_lengths.append(len(ctx))\n",
    "            for qa in par[\"qas\"]:\n",
    "                qa_cnt += 1\n",
    "                if not qa[\"answers\"]:\n",
    "                    empty_ans += 1\n",
    "                    bad_samples.append((\"EMPTY\", qa[\"id\"], qa[\"question\"][:40]))\n",
    "                    continue\n",
    "                ans = qa[\"answers\"][0]\n",
    "                text, start = ans[\"text\"], ans[\"answer_start\"]\n",
    "                ans_lengths.append(len(text))\n",
    "\n",
    "                if start < 0 or start + len(text) > len(ctx):\n",
    "                    out_of_bounds += 1\n",
    "                    bad_samples.append((\"OOB\", qa[\"id\"], qa[\"question\"][:40]))\n",
    "                elif ctx[start:start + len(text)] != text:\n",
    "                    mismatch += 1\n",
    "                    bad_samples.append((\"MISMATCH\", qa[\"id\"], qa[\"question\"][:40]))\n",
    "\n",
    "    print(f\"\\n📜 {path.name}\")\n",
    "    print(f\"  articles        : {art_cnt}\")\n",
    "    print(f\"  paragraphs      : {par_cnt}\")\n",
    "    print(f\"  QAs             : {qa_cnt}\")\n",
    "    print(f\"  ├── empty answer      : {empty_ans}\")\n",
    "    print(f\"  ├── start OOB         : {out_of_bounds}\")\n",
    "    print(f\"  └── substring mismatch: {mismatch}\")\n",
    "\n",
    "    if ctx_lengths:\n",
    "        print(f\"  context length  : max {max(ctx_lengths)}, \"\n",
    "              f\"avg {statistics.mean(ctx_lengths):.1f}\")\n",
    "    if ans_lengths:\n",
    "        print(f\"  answer length   : max {max(ans_lengths)}, \"\n",
    "              f\"avg {statistics.mean(ans_lengths):.1f}\")\n",
    "\n",
    "    if bad_samples:\n",
    "        print(\"\\n  ⚠️  첫 5개 오류 샘플\")\n",
    "        for kind, qid, qpreview in bad_samples[:5]:\n",
    "            print(f\"   [{kind}] {qid}  |  {qpreview}…\")\n",
    "\n",
    "# 실행\n",
    "validate(\"train\")\n",
    "validate(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832f81a-0122-498f-91c4-f53a1159b444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff09b1-407b-45d1-ac23-d031429bebed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a192461-79c9-44ce-8a74-13e7b2b7cedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
