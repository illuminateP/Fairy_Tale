{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10PNS_iPUkQ-",
        "outputId": "5ff1b77a-bead-46ae-b855-55a87a7cf1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import"
      ],
      "metadata": {
        "id": "WbzmNkyujZIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    BertTokenizerFast, BertForQuestionAnswering,\n",
        "    TrainingArguments, Trainer\n",
        ")"
      ],
      "metadata": {
        "id": "mbOs_7wZjYcn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ê²½ë¡œ ì§€ì •"
      ],
      "metadata": {
        "id": "JkVBvbEzjbVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/NLP (1)/training.json\"\n",
        "val_path   = \"/content/drive/MyDrive/NLP (1)/validation.json\"\n",
        "model_ckpt = \"beomi/kcbert-base\""
      ],
      "metadata": {
        "id": "CPkNkyJ2jcSX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "vwlYyRCZjfFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(model_ckpt)\n",
        "tokenizer.model_max_length = 300            # â† 300 ì´í•˜ë¡œ ê³ ì •\n",
        "MAX_LEN   = 256                             # ì‹¤ì œ ì…ë ¥ ê¸¸ì´\n",
        "DOC_STRIDE = 128                            # ìŠ¬ë¼ì´ë”© ìœˆë„\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained(model_ckpt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GcL6K20jg0-",
        "outputId": "94fb61bc-3c0d-4a4d-ea23-c37ddd07ce41"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "85X40DhMjiDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(json_path):\n",
        "    with open(json_path, encoding=\"utf-8\") as f:\n",
        "        raw = json.load(f)[\"data\"]\n",
        "    buf = []\n",
        "    for art in raw:\n",
        "        for para in art[\"paragraphs\"]:\n",
        "            ctx = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                for ans in qa[\"answers\"]:\n",
        "                    buf.append({\n",
        "                        \"id\"     : qa[\"id\"],\n",
        "                        \"context\": ctx,\n",
        "                        \"question\": qa[\"question\"],\n",
        "                        \"answer_text\" : ans[\"text\"],\n",
        "                        \"answer_start\": ans[\"answer_start\"],\n",
        "                    })\n",
        "    return buf\n",
        "\n",
        "train_samples = flatten(train_path)\n",
        "val_samples   = flatten(val_path)\n",
        "\n",
        "\n",
        "class KorQuAD(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.samples[idx]\n",
        "\n",
        "        enc = tokenizer(\n",
        "            ex[\"question\"], ex[\"context\"],\n",
        "            max_length=MAX_LEN,\n",
        "            truncation=\"only_second\",\n",
        "            stride=DOC_STRIDE,\n",
        "            return_overflowing_tokens=False,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        offset = enc.pop(\"offset_mapping\")[0]\n",
        "        ans_s, ans_e = ex[\"answer_start\"], ex[\"answer_start\"] + len(ex[\"answer_text\"])\n",
        "\n",
        "        tok_start = tok_end = None\n",
        "        for i, (s, e) in enumerate(offset):\n",
        "            if s <= ans_s < e: tok_start = i\n",
        "            if s <  ans_e <= e: tok_end   = i\n",
        "        # ë‹µì´ ì˜ë ¤ ë‚˜ê°€ë©´ ìƒ˜í”Œ drop\n",
        "        if tok_start is None or tok_end is None:\n",
        "            return self.__getitem__((idx+1)%len(self))   # ì¬ê·€ë¡œ ë‹¤ìŒ ìƒ˜í”Œ\n",
        "\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"start_positions\"] = torch.tensor(tok_start)\n",
        "        item[\"end_positions\"]   = torch.tensor(tok_end)\n",
        "        return item\n",
        "\n",
        "train_ds = KorQuAD(train_samples)\n",
        "val_ds   = KorQuAD(val_samples)"
      ],
      "metadata": {
        "id": "bOjcC4p2jlNS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í›ˆë ¨í•˜ê¸°"
      ],
      "metadata": {
        "id": "PcYBp6t6jl0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "YOFyva11PZO4",
        "outputId": "f58b3f0c-2600-4ef4-b29a-f54161836ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-2187846205>:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29020' max='29020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29020/29020 1:15:59, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.804200</td>\n",
              "      <td>0.794534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.519600</td>\n",
              "      <td>0.786869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.330600</td>\n",
              "      <td>0.984986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.182900</td>\n",
              "      <td>1.456881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.072800</td>\n",
              "      <td>1.734986</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=29020, training_loss=0.4284141697775159, metrics={'train_runtime': 4560.6064, 'train_samples_per_second': 50.901, 'train_steps_per_second': 6.363, 'total_flos': 3.032871455434752e+16, 'train_loss': 0.4284141697775159, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir       = \"/content/qa-out\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy    = \"epoch\",\n",
        "    learning_rate    = 5e-5,\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size  = 8,\n",
        "    num_train_epochs = 5,\n",
        "    weight_decay     = 0.01,\n",
        "    report_to        = \"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model         = model,\n",
        "    args          = training_args,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset  = val_ds,\n",
        "    tokenizer     = tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í‰ê°€í•˜ê¸°"
      ],
      "metadata": {
        "id": "EbN4vv4Cqmxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ flatten í›„ ê·¸ëŒ€ë¡œ ë³´ì¡´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "train_samples = flatten_korquad(train_path)   # â† answers í¬í•¨\n",
        "val_samples   = flatten_korquad(val_path)     # â† answers í¬í•¨\n",
        "\n",
        "# â€¦ Dataset/Trainer ì½”ë“œ â€¦\n",
        "\n",
        "# ----------  ì˜ˆì¸¡ ----------\n",
        "start_logits, end_logits = trainer.predict(val_ds).predictions\n",
        "\n",
        "# ----------  gold â”‚ pred ----------\n",
        "pred_texts = []\n",
        "for (s_log, e_log), enc in zip(zip(start_logits, end_logits), val_ds):\n",
        "    s = int(np.argmax(s_log));  e = int(np.argmax(e_log))\n",
        "    if e < s: e = s\n",
        "    pred_texts.append(\n",
        "        tokenizer.decode(enc[\"input_ids\"][s:e+1], skip_special_tokens=True).strip()\n",
        "    )\n",
        "\n",
        "gold_texts = [sample[\"answers\"][\"text\"][0] for sample in val_samples]  # â˜… ì—¬ê¸° ìˆ˜ì •\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "Ps7PpaQC2XNN",
        "outputId": "6dd8c041-1fc2-4bed-d947-5a5eda5c8bb1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'flatten_korquad' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1959053799>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ flatten í›„ ê·¸ëŒ€ë¡œ ë³´ì¡´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_korquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# â† answers í¬í•¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_samples\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mflatten_korquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_path\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# â† answers í¬í•¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# â€¦ Dataset/Trainer ì½”ë“œ â€¦\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'flatten_korquad' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "KYYgy7XbPpwC",
        "outputId": "66807281-dc3d-4295-fb51-ce1b4f4c30e6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'answers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3560849355>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mpred_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mgold_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# â”€â”€ 4. ì§€í‘œ ì§‘ê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-3560849355>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mpred_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mgold_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# â”€â”€ 4. ì§€í‘œ ì§‘ê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'answers'"
          ]
        }
      ],
      "source": [
        "import re, string, collections, numpy as np\n",
        "from itertools import zip_longest\n",
        "\n",
        "# â”€â”€ 1. SQuAD ê³µì‹ ì •ê·œí™” Â· í† í°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def _normalize(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = \"\".join(ch for ch in text if ch not in string.punctuation)\n",
        "    text = re.sub(r\"\\b(a|an|the)\\b\", \" \", text)        # ê´€ì‚¬ ì œê±°\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "def _tok(text: str):\n",
        "    return _normalize(text).split()\n",
        "\n",
        "# â”€â”€ 2. ë©”íŠ¸ë¦­ í•¨ìˆ˜ (Exact-Match / F1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def exact_match(pred: str, gold: str) -> int:\n",
        "    return int(_normalize(pred) == _normalize(gold))\n",
        "\n",
        "def f1_squad(pred: str, gold: str) -> float:\n",
        "    p_toks, g_toks = map(_tok, (pred, gold))\n",
        "    common = collections.Counter(p_toks) & collections.Counter(g_toks)\n",
        "    same = sum(common.values())\n",
        "    if same == 0:\n",
        "        return 0.0\n",
        "    precision = same / len(p_toks)\n",
        "    recall    = same / len(g_toks)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "# â”€â”€ 3. ê²€ì¦ ì…‹ inference â†’ ì˜ˆì¸¡ ë¬¸ìì—´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "start_logits, end_logits = trainer.predict(val_ds).predictions\n",
        "pred_texts = []\n",
        "for (s_log, e_log), sample in zip(zip(start_logits, end_logits), val_ds):\n",
        "    s = int(np.argmax(s_log));  e = int(np.argmax(e_log))\n",
        "    if e < s:                   e = s\n",
        "    text = tokenizer.decode(sample[\"input_ids\"][s:e+1],\n",
        "                            skip_special_tokens=True).strip()\n",
        "    pred_texts.append(text)\n",
        "\n",
        "gold_texts = [ex[\"answers\"][\"text\"][0] for ex in val_samples]\n",
        "\n",
        "# â”€â”€ 4. ì§€í‘œ ì§‘ê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "EM  = np.mean([exact_match(p, g) for p, g in zip_longest(pred_texts, gold_texts, fillvalue=\"\")])\n",
        "F1  = np.mean([f1_squad (p, g)   for p, g in zip_longest(pred_texts, gold_texts, fillvalue=\"\")])\n",
        "loss_val = trainer.evaluate(eval_dataset=val_ds).get(\"eval_loss\", float(\"nan\"))\n",
        "\n",
        "print(f\"ğŸ“Š  Validation â”‚ Loss={loss_val:.4f} â”‚ EM={EM:.4f} â”‚ F1={F1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ 5. ë°ëª¨: ì„ì˜ ìƒ˜í”Œë¡œ QA ì¸í„°ë™ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def ask_demo(idx:int=None):\n",
        "    \"\"\"idx ì—†ìœ¼ë©´ ê²€ì¦ì…‹ ì²« í•­ëª© ì‚¬ìš©\"\"\"\n",
        "    sample = val_samples[idx or 0]\n",
        "    context = sample[\"context\"]\n",
        "    print(\"â”€ ì§€ë¬¸ â”€\")\n",
        "    print(context[:400], \"...\" if len(context)>400 else \"\")  # ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ\n",
        "    print(\"\\n(â€» ìœ„ ì§€ë¬¸ ì¼ë¶€ë§Œ í‘œì‹œ - ì „ì²´ëŠ” model ì— ì…ë ¥ë©ë‹ˆë‹¤)\\n\")\n",
        "    q = input(\"ğŸ—¨ï¸  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
        "    enc = tokenizer(q, context,\n",
        "                    truncation=\"only_second\", max_length=512,\n",
        "                    return_offsets_mapping=False, return_tensors=\"pt\").to(trainer.model.device)\n",
        "    with torch.no_grad():\n",
        "        out = trainer.model(**enc)\n",
        "    s = int(out.start_logits.argmax()); e = int(out.end_logits.argmax())\n",
        "    if e < s: e = s\n",
        "    answer = tokenizer.decode(enc[\"input_ids\"][0][s:e+1],\n",
        "                              skip_special_tokens=True).strip()\n",
        "    print(f\"ğŸ¤–  ë‹µë³€: {answer}\")\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ\n",
        "ask_demo(2)     # ì…€ ì‹¤í–‰ í›„ ì½˜ì†”ì— ì§ˆë¬¸ ì…ë ¥"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc7668rz2FBu",
        "outputId": "71556b32-7c72-44fa-8349-8749276149ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”€ ì§€ë¬¸ â”€\n",
            "ì–´ëŠ ë§ˆì„ì— ë¦´ë¦¬ì—”íƒˆê³¼ êµ¬ìŠ¤íƒ€í”„ë¼ëŠ” í˜•ì œê°€ ì‚´ì•˜ì–´. ë¦´ë¦¬ì—”íƒˆê³¼ êµ¬ìŠ¤íƒ€í”„ëŠ” ì–´ë¦´ ë•Œë¶€í„° í•˜ëŠ˜ì„ ììœ ë¡­ê²Œ ë‚ ì•„ë‹¤ë‹ˆëŠ” ìƒˆë¥¼ ë¶€ëŸ¬ì›Œí–ˆì–´. ê·¸ë˜ì„œ ë‚ ë§ˆë‹¤ ì–¸ë•ì— ì˜¬ë¼ í•˜ëŠ˜ì„ ë‚˜ëŠ” ìƒˆë¥¼ êµ¬ê²½í•˜ê³¤ í–ˆì§€. \n",
            "\n",
            "(â€» ìœ„ ì§€ë¬¸ ì¼ë¶€ë§Œ í‘œì‹œ - ì „ì²´ëŠ” model ì— ì…ë ¥ë©ë‹ˆë‹¤)\n",
            "\n",
            "ğŸ—¨ï¸  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ë¦´ë¦¬ì—”íƒˆì€ ë¬´ì—‡ì„ ë¶€ëŸ¬ì›Œí–ˆë‚˜ìš”?\n",
            "ğŸ¤–  ë‹µë³€: í•˜ëŠ˜ì„ ììœ ë¡­ê²Œ ë‚ ì•„ë‹¤ë‹ˆëŠ” ìƒˆ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save"
      ],
      "metadata": {
        "id": "LeaIY-wEpr6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì›í•˜ëŠ” ê²½ë¡œì— ì €ì¥\n",
        "save_path = './my_model'  # colab ë‚´ ë””ë ‰í† ë¦¬ëª…\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "mfn1RH_appv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('my_model', 'zip', './my_model')"
      ],
      "metadata": {
        "id": "7yqDsxpnpx0o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}