{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcd972b-a0d4-42fe-859d-ba22ff38fa0f",
   "metadata": {},
   "source": [
    "### 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2dff119-409d-4af0-8eba-211310e12918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json, sys, re\n",
    "import shutil\n",
    "import collections\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import hashlib, itertools\n",
    "import csv   \n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing   import Union, Dict, List, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e67c9-925e-49cb-937c-52e406bfbf88",
   "metadata": {},
   "source": [
    "### unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4874a149-2e43-4ad2-aca0-857ce948bc4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[🚀] ./Origin/Sublabel 하위의 모든 .json을 ./converted/Sublabel로 평탄화(flatten)\n",
      "[✅ 완료] ./converted/Sublabel 에 2345개 복사 완료\n",
      "\n",
      "[🚀] ./Origin/Training/02.라벨링데이터 하위의 모든 .json을 ./converted/Training로 평탄화(flatten)\n",
      "[✅ 완료] ./converted/Training 에 1446개 복사 완료\n",
      "\n",
      "[🚀] ./Origin/Validation/02.라벨링데이터 하위의 모든 .json을 ./converted/Validation로 평탄화(flatten)\n",
      "[✅ 완료] ./converted/Validation 에 286개 복사 완료\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "    # (source, target)\n",
    "    ('./Origin/Sublabel',                 './converted/Sublabel'),\n",
    "    ('./Origin/Training/02.라벨링데이터',   './converted/Training'),\n",
    "    ('./Origin/Validation/02.라벨링데이터', './converted/Validation'),\n",
    "]\n",
    "\n",
    "for source_dir, target_dir in tasks:\n",
    "    copied_files = 0\n",
    "\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"[❌ 오류] 경로 '{source_dir}'가 없음\")\n",
    "        continue\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(f\"[🚀] {source_dir} 하위의 모든 .json을 {target_dir}로 평탄화(flatten)\")\n",
    "\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if root == target_dir:\n",
    "            continue\n",
    "        for file_name in files:\n",
    "            if not file_name.endswith('.json'):\n",
    "                continue\n",
    "\n",
    "            src = os.path.join(root, file_name)\n",
    "            dst = os.path.join(target_dir, file_name)\n",
    "\n",
    "            # ── 파일명 충돌 대비 ─────────────────────────\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(file_name)\n",
    "                i = 1\n",
    "                while os.path.exists(os.path.join(target_dir, f\"{base}_{i}{ext}\")):\n",
    "                    i += 1\n",
    "                new_name = f\"{base}_{i}{ext}\"\n",
    "                dst = os.path.join(target_dir, new_name)\n",
    "                print(f\"⚠️ 파일명 충돌: '{file_name}' → '{new_name}' 로 복사\")\n",
    "\n",
    "            try:\n",
    "                shutil.copy2(src, dst)    \n",
    "                copied_files += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[오류] '{src}' 복사 실패: {e}\")\n",
    "\n",
    "    print(f\"[✅ 완료] {target_dir} 에 {copied_files}개 복사 완료\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf8eec-9394-4148-b8ce-09e98158a539",
   "metadata": {},
   "source": [
    "### 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753a328d-34db-4e8e-a368-b1efa7a6b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 폴더 기준 훈련 데이터 고유권수 : 1446\n"
     ]
    }
   ],
   "source": [
    "isbns = set()\n",
    "for dirpath, _, filenames in os.walk('./Origin/Training'):\n",
    "    for fn in filenames:\n",
    "        if fn.lower().endswith('.json'):\n",
    "            with open(os.path.join(dirpath, fn), encoding='utf-8-sig') as f:\n",
    "                d = json.load(f)\n",
    "            books = d if isinstance(d, list) else [d]\n",
    "            for book in books:\n",
    "                isbns.add(str(book.get('isbn','')).strip())\n",
    "print(f\"원본 폴더 기준 훈련 데이터 고유권수 : {len(isbns)}\")  # 원본 폴더 기준 고유권수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61647f68-687d-4b15-8f49-0ab2233ce7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 폴더 기준 검증 데이터 고유권수 : 286\n"
     ]
    }
   ],
   "source": [
    "isbns = set()\n",
    "for dirpath, _, filenames in os.walk('./Origin/Validation'):\n",
    "    for fn in filenames:\n",
    "        if fn.lower().endswith('.json'):\n",
    "            with open(os.path.join(dirpath, fn), encoding='utf-8-sig') as f:\n",
    "                d = json.load(f)\n",
    "            books = d if isinstance(d, list) else [d]\n",
    "            for book in books:\n",
    "                isbns.add(str(book.get('isbn','')).strip())\n",
    "print(f\"원본 폴더 기준 검증 데이터 고유권수 : {len(isbns)}\")  # 원본 폴더 기준 고유권수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae55056-b635-44a0-9bc2-0314a8a8f90f",
   "metadata": {},
   "source": [
    "### 명시적 질문만 남기기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6bbb07-4d42-41c2-8b6e-b01c662ad3fb",
   "metadata": {},
   "source": [
    "★ 라벨링 데이터만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcb36b3-84a7-4f66-9d89-d2bb7816a8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70418f51d427472aaec80a368317912c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAINING:   0%|          | 0/1446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📜 training.json\n",
      "  articles   :  1442\n",
      "  paragraphs :  1442\n",
      "  QAs        : 47443\n",
      "    └─ 매칭 실패 : 1159\n",
      "  ── mismatch preview ──\n",
      "   • [9791128216893] ㅂ으로 시작되는 이름을 가진 과일 중 길쭉한 것은 무엇일까요?  →  바나나\n",
      "   • [9791128216893] 첫 글자를 보고 우리는 무엇을 해야 하나요?  →  첫 글자로 시작하는 이름을 찾아야 해요.\n",
      "   • [9791159424502] 아이들은 정원에서 어떻게 놀았나요?  →  신나게 놀았어요.\n",
      "   • [9791159424564] 꼬투리를 꺾었을 때 완두콩 오형제는 바깥세상으로 나갈 생각에 어떻게 했나요?  →  완두콩 오형제는 가슴이 두근거렸어요.\n",
      "   • [9791159424632] 빨간 모자는 어디를 걸어갔나요?  →  숲속\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc593e017f44193909a1749107b2182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VALIDATION:   0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📜 validation.json\n",
      "  articles   :   282\n",
      "  paragraphs :   282\n",
      "  QAs        :  5998\n",
      "    └─ 매칭 실패 : 174\n",
      "  ── mismatch preview ──\n",
      "   • [9791128216954] 빗방울이 창문을 두드린 이유는 무엇인가요?  →  집에만 있지말고 나와서 놀자고\n",
      "   • [9791159424700] 이상한 일은 어디에서 일어났나요?  →  할머니 집\n",
      "   • [9791165434724] 아이의 배변을 도와주려고 하는 이는 누구인가요?  →  야옹이\n",
      "   • [9791165435950] 책은 혼자 보는 것보다 어떻게 보는 것이 더 재미있을까요?  →  친구와 함께 보면 더 재미나요.\n",
      "   • [9791165436025] 엄마 배에 손을 대어 본 토리는 아기가 뭘 한다고 생각했나요?  →  깡총깡총 뛰나 봐요.\n",
      "\n",
      "🟢 All done! → cleaned/, formatted/  디렉터리 확인\n"
     ]
    }
   ],
   "source": [
    "# ╔═══════════════════════════════════════╗\n",
    "# ║   KorQuAD v1 말뭉치 + cleaned 본문   ║\n",
    "# ╚═══════════════════════════════════════╝\n",
    "# ───────────────── 경로 ─────────────────\n",
    "ROOT = Path(\"./converted\") ; ROOT.mkdir(exist_ok=True)     \n",
    "CLEAN_DIR = ROOT / \"cleaned\"       ; CLEAN_DIR.mkdir(exist_ok=True)\n",
    "FMT_DIR   = ROOT / \"formatted\"     ; FMT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "LBL_DIRS  = {\"training\": ROOT / \"training\",\n",
    "             \"validation\": ROOT / \"validation\"}\n",
    "\n",
    "# ───────────── ISBN 추출 util ────────────\n",
    "ISBN_RE = re.compile(r\"([0-9X]{9,13})(?=_?\\.json$)\", re.I)\n",
    "def isbn_from(path: Union[str, Path]) -> str:\n",
    "    path = Path(path)\n",
    "    m = ISBN_RE.search(path.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"ISBN not found in filename: {path.name}\")\n",
    "    return m.group(1)\n",
    "\n",
    "# ───────────── 정규화 & back-map ──────────\n",
    "PUNCT = re.compile(r\"[^\\w\\s]\")      # 필요하면 범위 조정\n",
    "def normalize_with_map(text: str) -> Tuple[str, List[int]]:\n",
    "    norm_chars, back = [], []\n",
    "    for i, ch in enumerate(text):\n",
    "        if PUNCT.match(ch):           # 문장부호 skip\n",
    "            continue\n",
    "        norm_chars.append(ch)\n",
    "        back.append(i)\n",
    "    return \"\".join(norm_chars), back\n",
    "\n",
    "def locate_answer(ctx: str, ans: str) -> Tuple[int, str] | Tuple[None, None]:\n",
    "    \"\"\"원문 ctx 에서 ans 위치 반환. 실패하면 punctuation-agnostic 재탐색.\"\"\"\n",
    "    pos = ctx.find(ans)\n",
    "    if pos != -1:\n",
    "        return pos, ans\n",
    "    # ― 재탐색 ―\n",
    "    ctx_norm, back = normalize_with_map(ctx)\n",
    "    ans_norm, _    = normalize_with_map(ans)\n",
    "    p2 = ctx_norm.find(ans_norm)\n",
    "    if p2 == -1:\n",
    "        return None, None\n",
    "    start = back[p2]\n",
    "    end   = back[p2 + len(ans_norm) - 1]\n",
    "    return start, ctx[start:end+1]\n",
    "\n",
    "# ───────────── Split 처리 함수 ────────────\n",
    "def process_split(split: str, src_root: Path) -> None:\n",
    "    korquad_data   : List[dict]      = []\n",
    "    total_qas = missed_qas = 0\n",
    "    missed_log : list[tuple] = []    # (isbn, Q, A)\n",
    "\n",
    "    for lbl_file in tqdm(list(src_root.glob(\"*.json\")), desc=split.upper()):\n",
    "        isbn = isbn_from(lbl_file)\n",
    "        book  = json.loads(lbl_file.read_text(encoding=\"utf-8-sig\"))\n",
    "\n",
    "        # ①  context  ─ 페이지 순서대로 모아 붙이기\n",
    "        paras = sorted(book.get(\"paragraphInfo\", []),\n",
    "                       key=lambda p: p.get(\"srcPage\", 0))\n",
    "        context = \"\\n\\n\".join(p.get(\"srcText\", \"\") for p in paras).strip()\n",
    "\n",
    "        # cleaned/<isbn>.json : 한 번만 저장\n",
    "        cleaned_path = CLEAN_DIR / f\"{isbn}.json\"\n",
    "        if not cleaned_path.exists():\n",
    "            cleaned_path.write_text(\n",
    "                json.dumps({\"isbn\": isbn,\n",
    "                            \"title\": book.get(\"title\", \"\"),\n",
    "                            \"text\": context},\n",
    "                           ensure_ascii=False, indent=2),\n",
    "                encoding=\"utf-8-sig\"\n",
    "            )\n",
    "\n",
    "        # ②  KorQuAD QA 변환  (명시적 질문만)\n",
    "        qas: List[dict] = []\n",
    "        for qa in itertools.chain.from_iterable(\n",
    "                p.get(\"queAnsPairInfo\", []) for p in paras):\n",
    "            if \"명시적\" not in qa.get(\"ansType\", \"\"):\n",
    "                continue\n",
    "            total_qas += 1\n",
    "            qid = f\"{isbn}-{hashlib.md5(qa['question'].encode()).hexdigest()[:8]}\"\n",
    "            ans_raw = qa.get(\"ansM1\", \"\").strip()\n",
    "            if not ans_raw:\n",
    "                missed_qas += 1\n",
    "                missed_log.append((isbn, qa[\"question\"], \"<EMPTY>\"))\n",
    "                continue\n",
    "            start, span = locate_answer(context, ans_raw)\n",
    "            if start is None:\n",
    "                missed_qas += 1\n",
    "                missed_log.append((isbn, qa[\"question\"], ans_raw))\n",
    "                continue\n",
    "            qas.append({\n",
    "                \"id\": qid,\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answers\": [{\"text\": span, \"answer_start\": start}]\n",
    "            })\n",
    "\n",
    "        if qas:          # QA 가 하나라도 있어야 포함\n",
    "            korquad_data.append({\n",
    "                \"title\": book.get(\"title\", isbn),\n",
    "                \"paragraphs\": [{\n",
    "                    \"context\": context,\n",
    "                    \"qas\": qas\n",
    "                }]\n",
    "            })\n",
    "\n",
    "    # ③  KorQuAD 파일 저장\n",
    "    out_file = FMT_DIR / f\"{split}.json\"\n",
    "    out_file.write_text(\n",
    "        json.dumps({\"version\": \"v1.0\", \"data\": korquad_data},\n",
    "                   ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    # ④  리포트\n",
    "    print(f\"\\n📜 {out_file.name}\")\n",
    "    print(f\"  articles   : {len(korquad_data):5d}\")\n",
    "    print(f\"  paragraphs : {len(korquad_data):5d}\")\n",
    "    print(f\"  QAs        : {total_qas:5d}\")\n",
    "    print(f\"    └─ 매칭 실패 : {missed_qas}\")\n",
    "    if missed_log:\n",
    "        print(\"  ── mismatch preview ──\")\n",
    "        for isbn, q, a in missed_log[:5]:\n",
    "            print(f\"   • [{isbn}] {q}  →  {a}\")\n",
    "\n",
    "# ────────────────────── main ──────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    for split, root in LBL_DIRS.items():\n",
    "        process_split(split, root)\n",
    "\n",
    "    print(\"\\n🟢 All done! → cleaned/, formatted/  디렉터리 확인\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ff4f6-64db-4ace-b631-c07fe18eb6a2",
   "metadata": {},
   "source": [
    "### formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bfc5a6a-fc6b-49a0-bd7f-5f2b83f4cb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a936397c01d34aebb6352b87ae537dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAINING:   0%|          | 0/1446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ training.json   paragraphs:  27662\n",
      "   total QA:  80043 | missed: 33615\n",
      "   ↪️  miss example:\n",
      "      • ISBN 9788961915915 p4 Q:어느 날 할머니가 꽃밭에 물을 주고 있는데 무슨 일이 벌어졌나요?  A:갑자기 하늘에서 커다란 우박이 떨어졌어요.\n",
      "      • ISBN 9788961915953 p4 Q:신의가 신이 나서 힘껏 달려간 이유는 무엇인가요?  A:아빠와 모처럼 나들이를 나왔기 때문이에요.\n",
      "      • ISBN 9788961916080 p5 Q:같은 공룡을 잡아먹는 육식 공룡은 어때 보여요?  A:무시무시해 보여요.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a79af157824666b80b2f50508ac189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VALIDATION:   0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ validation.json  paragraphs:   3249\n",
      "   total QA:  10005 | missed:  4179\n",
      "   ↪️  miss example:\n",
      "      • ISBN 9788961916028 p4 Q:내가 주는 힌트를 듣고 무엇을 해 볼 것이냐고 물어봤나요?  A:내가 누군지 맞혀 볼 것이냐고 물어봤어요.\n",
      "      • ISBN 9788961916103 p3 Q:릴리엔탈과 구스타프는 하늘의 새를 구경하기 위해 매일 무엇을 했나요?  A:언덕에 올랐어요.\n",
      "      • ISBN 9788961916103 p3 Q:형제는 자유롭게 하늘을 나는 새를 보며 어떤 감정이 들었나요?  A:새를 부러워했어요.\n",
      "\n",
      "🟢  Done — check formatted/miss_*.csv for full mismatch list.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════╗\n",
    "# ║  🟢  Cell 1   ║  KorQuAD 생성 + 매칭 실패 디버그 로그 ║\n",
    "# ╚══════════════════════════════════════════════════════╝\n",
    "\n",
    "ROOT          = Path(\"./converted\")\n",
    "TRAIN_LB      = ROOT / \"training\"\n",
    "VAL_LB        = ROOT / \"validation\"\n",
    "CLEAN_DIR     = ROOT / \"cleaned\"     \n",
    "OUT_DIR       = ROOT / \"formatted\"   \n",
    "\n",
    "ISBN_RE = re.compile(r\"(\\d{9,13}X?)_?(?=\\.json$)\")\n",
    "def isbn_from_name(p: Union[str, Path]) -> str:\n",
    "    m = ISBN_RE.search(Path(p).name)\n",
    "    if not m: raise ValueError(f\"ISBN not found in {p}\")\n",
    "    return m.group(1)\n",
    "\n",
    "def load_pool(dir_: Path) -> Dict[str, dict]:\n",
    "    return {isbn_from_name(f): json.loads(f.read_text(\"utf-8-sig\"))\n",
    "            for f in dir_.glob(\"*.json\")}\n",
    "\n",
    "# ───────── cleaned 파일 복사 ─────────\n",
    "def save_clean(isbn: str, book: dict):\n",
    "    pages = sorted(book[\"paragraphInfo\"], key=lambda x: x.get(\"srcPage\", 0))\n",
    "    (CLEAN_DIR / f\"{isbn}.json\").write_text(\n",
    "        json.dumps(\n",
    "            {\"isbn\": isbn,\n",
    "             \"title\": book.get(\"title\", \"\"),\n",
    "             \"pages\":[{\"page\":p.get(\"srcPage\"),\"text\":p[\"srcText\"]} for p in pages]},\n",
    "            ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "# ───────── 페이지→KorQuAD + miss 수집 ─────────\n",
    "def para_to_korquad(isbn: str, title: str, para: dict\n",
    ") -> Tuple[dict, List[dict]]:\n",
    "    ctx = para[\"srcText\"]\n",
    "    page = para.get(\"srcPage\")\n",
    "    qas, misses = [], []\n",
    "    for qa in para.get(\"queAnsPairInfo\", []):\n",
    "        ans = qa.get(\"ansM1\")\n",
    "        if not ans:\n",
    "            continue\n",
    "        start = ctx.find(ans)\n",
    "        if start == -1:\n",
    "            near = ctx[:300] if len(ctx) < 300 else ctx[max(0,start-15):start+len(ans)+15]\n",
    "            misses.append(\n",
    "                {\"isbn\":isbn, \"page\":page, \"question\":qa[\"question\"], \"answer\":ans,\n",
    "                 \"ctx_slice\":near.replace(\"\\n\",\" \"), \"reason\":\"not-found\"}\n",
    "            )\n",
    "            continue\n",
    "        qid = f\"{isbn}-{hashlib.md5(qa['question'].encode()).hexdigest()[:8]}\"\n",
    "        qas.append({\"id\":qid,\"question\":qa[\"question\"],\n",
    "                    \"answers\":[{\"text\":ans,\"answer_start\":start}]})\n",
    "    return ({\"title\":title,\"paragraphs\":[{\"context\":ctx,\"qas\":qas}]}, misses)\n",
    "\n",
    "# ───────── split 처리 ─────────\n",
    "def split_to_json(pool: Dict[str, dict], split_name: str):\n",
    "    korquad, miss_log = [], []\n",
    "    total_q, miss_q = 0, 0\n",
    "\n",
    "    for isbn, book in tqdm(pool.items(), desc=split_name.upper()):\n",
    "        save_clean(isbn, book)\n",
    "\n",
    "        for para in sorted(book[\"paragraphInfo\"],\n",
    "                           key=lambda x: x.get(\"srcPage\",0)):\n",
    "            total_q += len(para.get(\"queAnsPairInfo\",[]))\n",
    "            rec, miss = para_to_korquad(isbn, book.get(\"title\",\"\"), para)\n",
    "            miss_q   += len(miss)\n",
    "            miss_log.extend(miss)\n",
    "            if rec[\"paragraphs\"][0][\"qas\"]:\n",
    "                korquad.append(rec)\n",
    "\n",
    "    # KorQuAD json\n",
    "    out_json = OUT_DIR / f\"{split_name}.json\"\n",
    "    json.dump({\"version\":\"v1.0\",\"data\":korquad},\n",
    "              out_json.open(\"w\",encoding=\"utf-8\"),\n",
    "              ensure_ascii=False, indent=2)\n",
    "\n",
    "    # miss CSV\n",
    "    if miss_log:\n",
    "        miss_csv = OUT_DIR / f\"miss_{split_name}.csv\"\n",
    "        with miss_csv.open(\"w\",newline=\"\",encoding=\"utf-8-sig\") as f:\n",
    "            writer = csv.DictWriter(\n",
    "                f, fieldnames=[\"split\",\"isbn\",\"page\",\"question\",\"answer\",\"ctx_slice\",\"reason\"]\n",
    "            )\n",
    "            writer.writeheader()\n",
    "            for row in miss_log:\n",
    "                row[\"split\"]=split_name\n",
    "                writer.writerow(row)\n",
    "\n",
    "    # 요약 출력\n",
    "    print(f\"\\n✅ {out_json.name:14}  paragraphs:{len(korquad):7d}\")\n",
    "    print(f\"   total QA: {total_q:6d} | missed: {miss_q:5d}\")\n",
    "    if miss_log:\n",
    "        print(\"   ↪️  miss example:\")\n",
    "        for ex in miss_log[:3]:\n",
    "            print(f\"      • ISBN {ex['isbn']} p{ex['page']} Q:{ex['question']}  A:{ex['answer']}\")\n",
    "\n",
    "# ───────── main ─────────\n",
    "train_pool, val_pool = load_pool(TRAIN_LB), load_pool(VAL_LB)\n",
    "split_to_json(train_pool, \"training\")\n",
    "split_to_json(val_pool,   \"validation\")\n",
    "print(\"\\n🟢  Done — check formatted/miss_*.csv for full mismatch list.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c43529-b2a5-4063-880b-121a8eab4078",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547a1c76-d9e9-4953-b1f3-b114ed5915cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 training.json\n",
      "  QA total : 46,428\n",
      "  ✅ all offsets OK\n",
      "\n",
      "🔍 validation.json\n",
      "  QA total : 5,826\n",
      "  ✅ all offsets OK\n",
      "\n",
      "📂 cleaned dir : 1,732 files\n",
      "   KorQuAD ISBN referenced : 0\n",
      "   missing cleaned files   : 0 (none)\n",
      "\n",
      "🟢  verification done\n"
     ]
    }
   ],
   "source": [
    "# ╔════════════════════════════════════════════╗\n",
    "# ║  🟢  Cell 2   ║  KorQuAD / cleaned 검증    ║\n",
    "# ╚════════════════════════════════════════════╝\n",
    "import json, csv, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT       = Path(\"./converted\")\n",
    "FMT_DIR    = ROOT / \"formatted\"\n",
    "CLEAN_DIR  = ROOT / \"cleaned\"\n",
    "\n",
    "SPLITS = {\"training\":\"training.json\", \"validation\":\"validation.json\"}\n",
    "\n",
    "def verify_korquad(split:str, file:Path)-> List[Dict]:\n",
    "    \"\"\"answer_start 정합성 & 범위 체크 → 실패 row 목록 반환\"\"\"\n",
    "    bad : List[Dict] = []\n",
    "    data = json.loads(file.read_text(encoding=\"utf-8\"))\n",
    "    for art in data[\"data\"]:\n",
    "        title  = art.get(\"title\",\"\")\n",
    "        for para in art[\"paragraphs\"]:\n",
    "            ctx    = para[\"context\"]\n",
    "            ctxlen = len(ctx)\n",
    "            for qa in para[\"qas\"]:\n",
    "                for ans in qa[\"answers\"]:\n",
    "                    text  = ans[\"text\"]\n",
    "                    start = ans[\"answer_start\"]\n",
    "                    # ① 답이 context에 존재?\n",
    "                    found_at = ctx.find(text)\n",
    "                    # ② start 범위, slice 일치\n",
    "                    slice_ok = 0 <= start <= ctxlen-len(text) and ctx[start:start+len(text)]==text\n",
    "                    if slice_ok and found_at==start:\n",
    "                        continue   # 정상\n",
    "                    # --- 문제 기록\n",
    "                    reason  = (\"not-found\"      if found_at==-1 else\n",
    "                               \"misalign\"      if found_at!=start else\n",
    "                               \"slice-mismatch\")\n",
    "                    bad.append({\n",
    "                        \"split\":split,\n",
    "                        \"title\":title,\n",
    "                        \"id\":qa[\"id\"],\n",
    "                        \"question\":qa[\"question\"],\n",
    "                        \"answer\":text,\n",
    "                        \"start\":start,\n",
    "                        \"found_at\":found_at,\n",
    "                        \"reason\":reason,\n",
    "                        \"ctx_slice\":ctx[max(0,start-20):min(ctxlen,start+len(text)+20)].replace(\"\\n\",\" \")\n",
    "                    })\n",
    "    return bad\n",
    "\n",
    "def save_csv(rows:List[Dict], out:Path):\n",
    "    if not rows: return\n",
    "    with out.open(\"w\",newline=\"\",encoding=\"utf-8-sig\") as f:\n",
    "        w=csv.DictWriter(f,fieldnames=list(rows[0]))\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "\n",
    "# ───── 1) KorQuAD 파일 검사 ─────\n",
    "isbn_in_kqa=set()\n",
    "for split,fname in SPLITS.items():\n",
    "    fpath = FMT_DIR / fname\n",
    "    if not fpath.exists(): \n",
    "        print(f\"⚠️  {fname} not found\"); continue\n",
    "    bad = verify_korquad(split,fpath)\n",
    "    save_csv(bad, FMT_DIR/f\"bad_{split}.csv\")\n",
    "    # 통계\n",
    "    total   = sum( len(p[\"qas\"]) for a in json.loads(fpath.read_text(encoding=\"utf-8\"))[\"data\"]\n",
    "                                for p in a[\"paragraphs\"])\n",
    "    print(f\"\\n🔍 {fname}\")\n",
    "    print(f\"  QA total : {total:,}\")\n",
    "    print(f\"  bad rows : {len(bad):,}   ⇢  saved to bad_{split}.csv\" if bad else \"  ✅ all offsets OK\")\n",
    "    if bad:                        # 화면 예시\n",
    "        for row in bad[:3]:\n",
    "            print(f\"   • {row['id']} | reason:{row['reason']} \"\n",
    "                  f\" start {row['start']} / found {row['found_at']} : “{row['answer']}”\")\n",
    "\n",
    "    # ISBN 추적\n",
    "    for art in json.loads(fpath.read_text(encoding=\"utf-8\"))[\"data\"]:\n",
    "        tit = art.get(\"title\",\"\")\n",
    "        m = re.fullmatch(r\"\\d{9,13}X?\", tit)\n",
    "        if m: isbn_in_kqa.add(m.group())\n",
    "\n",
    "# ───── 2) cleaned 파일 존재 체크 ─────\n",
    "missing = [ {\"isbn\":isbn} for isbn in sorted(isbn_in_kqa)\n",
    "            if not (CLEAN_DIR/f\"{isbn}.json\").exists() ]\n",
    "save_csv(missing, FMT_DIR/\"missing_cleaned.csv\")\n",
    "\n",
    "print(f\"\\n📂 cleaned dir : {len(list(CLEAN_DIR.glob('*.json'))):,} files\")\n",
    "print(f\"   KorQuAD ISBN referenced : {len(isbn_in_kqa):,}\")\n",
    "print(f\"   missing cleaned files   : {len(missing):,}\"\n",
    "      + (f\"  ⇢ saved to missing_cleaned.csv\" if missing else \" (none)\"))\n",
    "\n",
    "print(\"\\n🟢  verification done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8d1ab-2a7d-4fee-84c6-7931ad5b2eb1",
   "metadata": {},
   "source": [
    "### Metatdata 생성 (for web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "434a99aa-8dec-4ded-8403-1a7c16a06d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾  meta\\book_meta_all_training.json  (1446 books)\n",
      "💾  meta\\book_meta_all_validation.json  (286 books)\n",
      "🟢  meta files created\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════╗\n",
    "# ║  🟢  Cell 3 – meta 파일 2종   ║\n",
    "# ╚══════════════════════════════╝\n",
    "\n",
    "ROOT      = Path(\"./converted\")\n",
    "TRAIN_LB  = ROOT / \"training\"\n",
    "VAL_LB    = ROOT / \"validation\"\n",
    "META_DIR   = ROOT / \"meta\"; META_DIR.mkdir(exist_ok=True)\n",
    "ISBN_RE = re.compile(r\"([0-9X]{9,13})(?=_?\\.json$)\")\n",
    "\n",
    "def isbn_from_name(p:Path)->str:\n",
    "    m = ISBN_RE.search(p.name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def load_pool(p_dir:Path):\n",
    "    return {isbn_from_name(f):json.loads(f.read_text(\"utf-8-sig\"))\n",
    "            for f in p_dir.glob(\"*.json\")}\n",
    "\n",
    "def to_meta_dict(isbn:str, book:dict):\n",
    "    meta = {\n",
    "        \"isbn\"         : isbn,\n",
    "        \"title\"        : book.get(\"title\",\"\").strip(),\n",
    "        \"author\"       : book.get(\"author\"),\n",
    "        \"illustrator\"  : book.get(\"illustrator\"),\n",
    "        \"publisher\"    : book.get(\"publisher\"),\n",
    "        \"publishedYear\": book.get(\"publishedYear\"),\n",
    "        \"readAge\"      : book.get(\"readAge\"),\n",
    "        \"classification\": book.get(\"classification\"),\n",
    "    }\n",
    "    # 빈 값은 제거\n",
    "    return {k:v for k,v in meta.items() if v}\n",
    "\n",
    "def build_meta(pool:dict, split:str):\n",
    "    metas = [to_meta_dict(i,bk) for i,bk in pool.items()]\n",
    "    out   = META_DIR / f\"book_meta_all_{split}.json\"\n",
    "    out.write_text(json.dumps(metas, ensure_ascii=False, indent=2),\n",
    "                   encoding=\"utf-8\")\n",
    "    print(f\"💾  {out.relative_to(ROOT)}  ({len(metas)} books)\")\n",
    "\n",
    "train_pool = load_pool(TRAIN_LB)\n",
    "val_pool   = load_pool(VAL_LB)\n",
    "\n",
    "build_meta(train_pool, \"training\")\n",
    "build_meta(val_pool,   \"validation\")\n",
    "print(\"🟢  meta files created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c364babf-088a-4441-96a9-939ba0aae91a",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4579b868-55b8-4e75-a704-13c66f787cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ converted\\meta\\book_meta_all.json 저장 완료 (1732 권)\n"
     ]
    }
   ],
   "source": [
    "# 파일 경로\n",
    "TRAIN_META = Path('./converted/meta/book_meta_all_training.json')\n",
    "VAL_META   = Path('./converted/meta/book_meta_all_validation.json')\n",
    "OUT_META   = Path('./converted/meta/book_meta_all.json')\n",
    "\n",
    "# 로드\n",
    "with TRAIN_META.open(encoding='utf-8-sig') as f:\n",
    "    train_meta = json.load(f)\n",
    "with VAL_META.open(encoding='utf-8-sig') as f:\n",
    "    val_meta = json.load(f)\n",
    "\n",
    "# ISBN 중복 제거 (training 우선, 없으면 validation)\n",
    "meta_dict = {item['isbn']: item for item in val_meta}\n",
    "meta_dict.update({item['isbn']: item for item in train_meta})  # train이 우선\n",
    "\n",
    "# 저장\n",
    "with OUT_META.open('w', encoding='utf-8-sig') as f:\n",
    "    json.dump(list(meta_dict.values()), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ {OUT_META} 저장 완료 ({len(meta_dict)} 권)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e670d6-c7ac-492b-8e92-a4f9c3bb909e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175db0f-ffb7-452f-bdc8-9dac120ddce7",
   "metadata": {},
   "source": [
    "# 끝 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19968f2-10fc-4a65-9252-a44e4caa5780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
