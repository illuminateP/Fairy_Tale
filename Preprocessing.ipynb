{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2dff119-409d-4af0-8eba-211310e12918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753a328d-34db-4e8e-a368-b1efa7a6b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ í´ë” ê¸°ì¤€ ê³ ìœ ê¶Œìˆ˜ : 1446\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "isbns = set()\n",
    "for dirpath, _, filenames in os.walk('./datasets/Training'):\n",
    "    for fn in filenames:\n",
    "        if fn.lower().endswith('.json'):\n",
    "            with open(os.path.join(dirpath, fn), encoding='utf-8-sig') as f:\n",
    "                d = json.load(f)\n",
    "            books = d if isinstance(d, list) else [d]\n",
    "            for book in books:\n",
    "                isbns.add(str(book.get('isbn','')).strip())\n",
    "print(f\"ì›ë³¸ í´ë” ê¸°ì¤€ ê³ ìœ ê¶Œìˆ˜ : {len(isbns)}\")  # ì›ë³¸ í´ë” ê¸°ì¤€ ê³ ìœ ê¶Œìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61647f68-687d-4b15-8f49-0ab2233ce7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ í´ë” ê¸°ì¤€ ê³ ìœ ê¶Œìˆ˜ : 286\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "isbns = set()\n",
    "for dirpath, _, filenames in os.walk('./datasets/Validation'):\n",
    "    for fn in filenames:\n",
    "        if fn.lower().endswith('.json'):\n",
    "            with open(os.path.join(dirpath, fn), encoding='utf-8-sig') as f:\n",
    "                d = json.load(f)\n",
    "            books = d if isinstance(d, list) else [d]\n",
    "            for book in books:\n",
    "                isbns.add(str(book.get('isbn','')).strip())\n",
    "print(f\"ì›ë³¸ í´ë” ê¸°ì¤€ ê³ ìœ ê¶Œìˆ˜ : {len(isbns)}\")  # ì›ë³¸ í´ë” ê¸°ì¤€ ê³ ìœ ê¶Œìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0d7065-480e-4351-91db-b40b9e50cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec43cf3a-7249-46e5-9499-8016ce89cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.lower().endswith('.json'):\n",
    "            json_files.append(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7293c92d-b18c-4673-b7d1-7de5be8b668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn_title_set = set()\n",
    "isbn_meta_dict = {}\n",
    "\n",
    "for filepath in json_files:\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8-sig') as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):  # í˜¹ì‹œ ë¦¬ìŠ¤íŠ¸í˜•ì´ë©´\n",
    "            items = data\n",
    "        else:\n",
    "            items = [data]\n",
    "        for book in items:\n",
    "            isbn = str(book.get('isbn', '')).strip()\n",
    "            title = book.get('title', '').strip()\n",
    "            if isbn and title:\n",
    "                isbn_title_set.add((isbn, title))\n",
    "                # ë©”íƒ€ë§Œ ë‚¨ê¸°ê¸°\n",
    "                meta = {\n",
    "                    \"isbn\": isbn,\n",
    "                    \"title\": title,\n",
    "                    \"author\": book.get('author', '').strip(),\n",
    "                    \"illustrator\": book.get('illustrator', '').strip(),\n",
    "                    \"readAge\": book.get('readAge', '').strip(),\n",
    "                    \"publishedYear\": book.get('publishedYear', ''),\n",
    "                    \"publisher\": book.get('publisher', '').strip(),\n",
    "                    \"classification\": book.get('classification', '').strip()\n",
    "                }\n",
    "                isbn_meta_dict[isbn] = meta\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {filepath}, {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df94db-d901-45e5-9df4-c249a678ba6a",
   "metadata": {},
   "source": [
    "### csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fd7d58-c08c-4cd9-b1f8-ba19c9349e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. isbn-title csv ì €ì¥ (1732 í–‰)\n"
     ]
    }
   ],
   "source": [
    "# 1. isbn + title csv\n",
    "df_titles = pd.DataFrame(list(isbn_title_set), columns=['isbn', 'title']).sort_values('isbn')\n",
    "df_titles.to_csv(os.path.join(root_dir, 'isbn_title_only.csv'), index=False, encoding='utf-8-sig')\n",
    "print(f\"1. isbn-title csv ì €ì¥ ({len(df_titles)} í–‰)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad3819-086c-4d21-877e-216dba70fb2c",
   "metadata": {},
   "source": [
    "### json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f163bcf6-13eb-4888-af91-bd47275caeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. isbnë³„ ë©”íƒ€ json ì €ì¥ (1732 íŒŒì¼)\n"
     ]
    }
   ],
   "source": [
    "# 2. isbnë³„ ë©”íƒ€ë°ì´í„° json íŒŒì¼\n",
    "meta_dir = os.path.join(root_dir, 'isbn_meta')\n",
    "os.makedirs(meta_dir, exist_ok=True)\n",
    "for isbn, meta in isbn_meta_dict.items():\n",
    "    with open(os.path.join(meta_dir, f\"{isbn}.json\"), 'w', encoding='utf-8-sig') as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"2. isbnë³„ ë©”íƒ€ json ì €ì¥ ({len(isbn_meta_dict)} íŒŒì¼)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d4e89-4ef6-44de-9b3f-a2db2b061bcd",
   "metadata": {},
   "source": [
    "### meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d8a17f-08f8-4413-9ebc-c846b856af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. ì „ì²´ í†µí•© ë©”íƒ€ json ì €ì¥ (1732ê¶Œ)\n"
     ]
    }
   ],
   "source": [
    "# 3. ì „ì²´ í†µí•© ë©”íƒ€ json\n",
    "all_meta_path = os.path.join(root_dir, 'all_isbn_meta.json')\n",
    "with open(all_meta_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump(list(isbn_meta_dict.values()), f, ensure_ascii=False, indent=2)\n",
    "print(f\"3. ì „ì²´ í†µí•© ë©”íƒ€ json ì €ì¥ ({len(isbn_meta_dict)}ê¶Œ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516311bc-e873-4173-9a08-dfb48861c291",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e1e5b-3c89-46bf-942b-96bdc8283aff",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66ea76cb-32ff-488f-8473-1c84ad18e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ isbn ê³ ìœ ê¶Œìˆ˜: 1446\n",
      "ëª…ì‹œì  ì§ˆë¬¸ 1ê°œ ì´ìƒ ìˆëŠ” ì±… ê¶Œìˆ˜: 1443\n",
      "ëª…ì‹œì  ì§ˆë¬¸ 0ê°œì¸ ì±… ê¶Œìˆ˜: 3\n",
      "[Sublabel ì—†ìŒ] 9791159420214\n",
      "[Sublabel ì—†ìŒ] 9791186922972\n",
      "[Sublabel ì—†ìŒ] 9788961914314\n",
      "Sublabel ë³µì‚¬: 1440ê°œ ì„±ê³µ, 3ê°œ ì‹¤íŒ¨\n",
      "[ìµœì¢…] ì „ì²´ ê³ ìœ  isbn: 1446\n",
      "[ìµœì¢…] ëª…ì‹œì  ì§ˆë¬¸ ìˆëŠ” ì±…: 1443\n",
      "[ìµœì¢…] ëª…ì‹œì  ì§ˆë¬¸ ì—†ëŠ” ì±…: 3\n",
      "ëª¨ë“  ì „ì²˜ë¦¬/ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- ê²½ë¡œ ì„¤ì • ---\n",
    "root_dir = './datasets/Training'\n",
    "sublabel_dir = './datasets/Sublabel'\n",
    "converted_root = './converted/training'\n",
    "converted_sublabel = './converted/Sublabel'\n",
    "json_dir = os.path.join(converted_root, 'json')\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "os.makedirs(converted_sublabel, exist_ok=True)\n",
    "\n",
    "# --- 1. isbnë³„ ë©”íƒ€/ì§ˆë¬¸ ì§‘ê³„ ---\n",
    "isbn_info = {}\n",
    "isbn_explicit = defaultdict(bool)\n",
    "\n",
    "json_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.lower().endswith('.json'):\n",
    "            json_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "for src_path in json_files:\n",
    "    with open(src_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    books = data if isinstance(data, list) else [data]\n",
    "    for book in books:\n",
    "        isbn = str(book.get('isbn', '')).strip()\n",
    "        title = book.get('title', '').strip()\n",
    "        if not isbn:\n",
    "            continue\n",
    "        # ë©”íƒ€ ì •ë³´ ìµœì´ˆ 1íšŒë§Œ ê¸°ë¡\n",
    "        if isbn not in isbn_info:\n",
    "            meta = {\"isbn\": isbn, \"title\": title}\n",
    "            for key in [\"author\", \"illustrator\", \"readAge\", \"publishedYear\", \"publisher\", \"classification\"]:\n",
    "                if key in book and book[key]:\n",
    "                    meta[key] = str(book[key]).strip()\n",
    "            isbn_info[isbn] = meta\n",
    "        # ëª…ì‹œì  ì§ˆë¬¸ ìˆëŠ”ì§€ ì²´í¬\n",
    "        for para in book.get('paragraphInfo', []):\n",
    "            for qa in para.get('queAnsPairInfo', []):\n",
    "                if 'ëª…ì‹œì ' in qa.get('ansType', ''):\n",
    "                    isbn_explicit[isbn] = True\n",
    "\n",
    "isbn_all_set = set(isbn_info.keys())\n",
    "isbn_explicit_set = set(isbn for isbn, flag in isbn_explicit.items() if flag)\n",
    "isbn_removed_set = isbn_all_set - isbn_explicit_set\n",
    "\n",
    "print(f\"ì „ì²´ isbn ê³ ìœ ê¶Œìˆ˜: {len(isbn_all_set)}\")\n",
    "print(f\"ëª…ì‹œì  ì§ˆë¬¸ 1ê°œ ì´ìƒ ìˆëŠ” ì±… ê¶Œìˆ˜: {len(isbn_explicit_set)}\")\n",
    "print(f\"ëª…ì‹œì  ì§ˆë¬¸ 0ê°œì¸ ì±… ê¶Œìˆ˜: {len(isbn_removed_set)}\")\n",
    "\n",
    "# --- 2. ë³€í™˜ json/meta ---\n",
    "meta_list = []\n",
    "meta_dict = {}\n",
    "\n",
    "for src_path in json_files:\n",
    "    with open(src_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    books = data if isinstance(data, list) else [data]\n",
    "    new_books = []\n",
    "    for book in books:\n",
    "        isbn = str(book.get('isbn', '')).strip()\n",
    "        if isbn not in isbn_explicit_set:\n",
    "            continue\n",
    "        # ëª…ì‹œì  ì§ˆë¬¸ë§Œ ë‚¨ê¸´ë‹¤\n",
    "        book_copy = dict(book)\n",
    "        new_paragraphs = []\n",
    "        for para in book.get('paragraphInfo', []):\n",
    "            new_para = dict(para)\n",
    "            new_qapairs = []\n",
    "            for qa in para.get('queAnsPairInfo', []):\n",
    "                if 'ëª…ì‹œì ' in qa.get('ansType', ''):\n",
    "                    new_qapairs.append(qa)\n",
    "            new_para['queAnsPairInfo'] = new_qapairs\n",
    "            new_para['queAnsPairInfoCount'] = len(new_qapairs)\n",
    "            new_paragraphs.append(new_para)\n",
    "        book_copy['paragraphInfo'] = new_paragraphs\n",
    "        new_books.append(book_copy)\n",
    "    if new_books:\n",
    "        rel_path = os.path.relpath(src_path, root_dir)\n",
    "        save_path = os.path.join(converted_root, rel_path)\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, 'w', encoding='utf-8-sig') as f:\n",
    "            if isinstance(data, list):\n",
    "                json.dump(new_books, f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                json.dump(new_books[0], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "for isbn in sorted(isbn_explicit_set):\n",
    "    meta_dict[isbn] = isbn_info[isbn]\n",
    "    meta_list.append(isbn_info[isbn])\n",
    "\n",
    "# --- 3. Sublabel ë³µì‚¬(í•˜ìœ„í´ë” ì „ì²´ì—ì„œ isbnì— ë§ëŠ” íŒŒì¼ ì°¾ê¸°) ---\n",
    "def find_sublabel_file(sublabel_dir, isbn):\n",
    "    for dirpath, _, filenames in os.walk(sublabel_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(f\"{isbn}.json\"):\n",
    "                return os.path.join(dirpath, filename)\n",
    "    return None\n",
    "\n",
    "copy_ok, copy_fail = 0, 0\n",
    "for isbn in isbn_explicit_set:\n",
    "    src_full = find_sublabel_file(sublabel_dir, isbn)\n",
    "    dst_full = os.path.join(converted_sublabel, f\"{isbn}.json\")\n",
    "    if src_full and os.path.exists(src_full):\n",
    "        shutil.copy2(src_full, dst_full)\n",
    "        copy_ok += 1\n",
    "    else:\n",
    "        print(f\"[Sublabel ì—†ìŒ] {isbn}\")\n",
    "        copy_fail += 1\n",
    "print(f\"Sublabel ë³µì‚¬: {copy_ok}ê°œ ì„±ê³µ, {copy_fail}ê°œ ì‹¤íŒ¨\")\n",
    "\n",
    "# --- 4. ê²°ê³¼ ì €ì¥ ---\n",
    "meta_simple_path = os.path.join(json_dir, 'book_titles_by_isbn.json')\n",
    "with open(meta_simple_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump([{\"isbn\": m[\"isbn\"], \"title\": m[\"title\"]} for m in meta_list], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "meta_all_path = os.path.join(json_dir, 'book_meta_all.json')\n",
    "with open(meta_all_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump(meta_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "meta_books_dir = os.path.join(json_dir, 'books_by_isbn')\n",
    "os.makedirs(meta_books_dir, exist_ok=True)\n",
    "for isbn, meta in meta_dict.items():\n",
    "    meta_path = os.path.join(meta_books_dir, f'{isbn}.json')\n",
    "    with open(meta_path, 'w', encoding='utf-8-sig') as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "removed_path = os.path.join(json_dir, 'removed_books.csv')\n",
    "pd.DataFrame([isbn_info[i] for i in sorted(isbn_removed_set)]).to_csv(removed_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"[ìµœì¢…] ì „ì²´ ê³ ìœ  isbn: {len(isbn_all_set)}\")\n",
    "print(f\"[ìµœì¢…] ëª…ì‹œì  ì§ˆë¬¸ ìˆëŠ” ì±…: {len(isbn_explicit_set)}\")\n",
    "print(f\"[ìµœì¢…] ëª…ì‹œì  ì§ˆë¬¸ ì—†ëŠ” ì±…: {len(isbn_removed_set)}\")\n",
    "print(f\"ëª¨ë“  ì „ì²˜ë¦¬/ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcd0f4-dec3-4ca4-a4a5-6001c9df4e66",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "463647ac-7950-4f0a-90c6-857111e232fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ isbn ê³ ìœ ê¶Œìˆ˜: 286\n",
      "ëª…ì‹œì  ì§ˆë¬¸ 1ê°œ ì´ìƒ ìˆëŠ” ì±… ê¶Œìˆ˜: 284\n",
      "ëª…ì‹œì  ì§ˆë¬¸ 0ê°œì¸ ì±… ê¶Œìˆ˜: 2\n",
      "[Sublabel ì—†ìŒ] 9791159420160\n",
      "[Sublabel ì—†ìŒ] 9791159420054\n",
      "[Sublabel ì—†ìŒ] 9791128208904\n",
      "Sublabel ë³µì‚¬: 281ê°œ ì„±ê³µ, 3ê°œ ì‹¤íŒ¨\n",
      "[ìµœì¢…] ì „ì²´ ê³ ìœ  isbn: 286\n",
      "[ìµœì¢…] ëª…ì‹œì  ì§ˆë¬¸ ìˆëŠ” ì±…: 284\n",
      "[ìµœì¢…] ëª…ì‹œì  ì§ˆë¬¸ ì—†ëŠ” ì±…: 2\n",
      "ëª¨ë“  ì „ì²˜ë¦¬/ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- ê²½ë¡œ ì„¤ì • ---\n",
    "root_dir = './datasets/Validation'\n",
    "sublabel_dir = './datasets/Sublabel'\n",
    "converted_root = './converted/validation'\n",
    "converted_sublabel = './converted/Sublabel'\n",
    "json_dir = os.path.join(converted_root, 'json')\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "os.makedirs(converted_sublabel, exist_ok=True)\n",
    "\n",
    "# --- 1. isbnë³„ ë©”íƒ€/ì§ˆë¬¸ ì§‘ê³„ ---\n",
    "isbn_info = {}\n",
    "isbn_explicit = defaultdict(bool)\n",
    "\n",
    "json_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.lower().endswith('.json'):\n",
    "            json_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "for src_path in json_files:\n",
    "    with open(src_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    books = data if isinstance(data, list) else [data]\n",
    "    for book in books:\n",
    "        isbn = str(book.get('isbn', '')).strip()\n",
    "        title = book.get('title', '').strip()\n",
    "        if not isbn:\n",
    "            continue\n",
    "        # ë©”íƒ€ ì •ë³´ ìµœì´ˆ 1íšŒë§Œ ê¸°ë¡\n",
    "        if isbn not in isbn_info:\n",
    "            meta = {\"isbn\": isbn, \"title\": title}\n",
    "            for key in [\"author\", \"illustrator\", \"readAge\", \"publishedYear\", \"publisher\", \"classification\"]:\n",
    "                if key in book and book[key]:\n",
    "                    meta[key] = str(book[key]).strip()\n",
    "            isbn_info[isbn] = meta\n",
    "        # ëª…ì‹œì  ì§ˆë¬¸ ìˆëŠ”ì§€ ì²´í¬\n",
    "        for para in book.get('paragraphInfo', []):\n",
    "            for qa in para.get('queAnsPairInfo', []):\n",
    "                if 'ëª…ì‹œì ' in qa.get('ansType', ''):\n",
    "                    isbn_explicit[isbn] = True\n",
    "\n",
    "isbn_all_set = set(isbn_info.keys())\n",
    "isbn_explicit_set = set(isbn for isbn, flag in isbn_explicit.items() if flag)\n",
    "isbn_removed_set = isbn_all_set - isbn_explicit_set\n",
    "\n",
    "print(f\"ì „ì²´ isbn ê³ ìœ ê¶Œìˆ˜: {len(isbn_all_set)}\")\n",
    "print(f\"ëª…ì‹œì  ì§ˆë¬¸ 1ê°œ ì´ìƒ ìˆëŠ” ì±… ê¶Œìˆ˜: {len(isbn_explicit_set)}\")\n",
    "print(f\"ëª…ì‹œì  ì§ˆë¬¸ 0ê°œì¸ ì±… ê¶Œìˆ˜: {len(isbn_removed_set)}\")\n",
    "\n",
    "# --- 2. ë³€í™˜ json/meta ---\n",
    "meta_list = []\n",
    "meta_dict = {}\n",
    "\n",
    "for src_path in json_files:\n",
    "    with open(src_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    books = data if isinstance(data, list) else [data]\n",
    "    new_books = []\n",
    "    for book in books:\n",
    "        isbn = str(book.get('isbn', '')).strip()\n",
    "        if isbn not in isbn_explicit_set:\n",
    "            continue\n",
    "        # ëª…ì‹œì  ì§ˆë¬¸ë§Œ ë‚¨ê¸´ë‹¤\n",
    "        book_copy = dict(book)\n",
    "        new_paragraphs = []\n",
    "        for para in book.get('paragraphInfo', []):\n",
    "            new_para = dict(para)\n",
    "            new_qapairs = []\n",
    "            for qa in para.get('queAnsPairInfo', []):\n",
    "                if 'ëª…ì‹œì ' in qa.get('ansType', ''):\n",
    "                    new_qapairs.append(qa)\n",
    "            new_para['queAnsPairInfo'] = new_qapairs\n",
    "            new_para['queAnsPairInfoCount'] = len(new_qapairs)\n",
    "            new_paragraphs.append(new_para)\n",
    "        book_copy['paragraphInfo'] = new_paragraphs\n",
    "        new_books.append(book_copy)\n",
    "    if new_books:\n",
    "        rel_path = os.path.relpath(src_path, root_dir)\n",
    "        save_path = os.path.join(converted_root, rel_path)\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, 'w', encoding='utf-8-sig') as f:\n",
    "            if isinstance(data, list):\n",
    "                json.dump(new_books, f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                json.dump(new_books[0], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "for isbn in sorted(isbn_explicit_set):\n",
    "    meta_dict[isbn] = isbn_info[isbn]\n",
    "    meta_list.append(isbn_info[isbn])\n",
    "\n",
    "# --- 3. Sublabel ë³µì‚¬(í•˜ìœ„í´ë” ì „ì²´ì—ì„œ isbnì— ë§ëŠ” íŒŒì¼ ì°¾ê¸°) ---\n",
    "def find_sublabel_file(sublabel_dir, isbn):\n",
    "    for dirpath, _, filenames in os.walk(sublabel_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(f\"{isbn}.json\"):\n",
    "                return os.path.join(dirpath, filename)\n",
    "    return None\n",
    "\n",
    "copy_ok, copy_fail = 0, 0\n",
    "for isbn in isbn_explicit_set:\n",
    "    src_full = find_sublabel_file(sublabel_dir, isbn)\n",
    "    dst_full = os.path.join(converted_sublabel, f\"{isbn}.json\")\n",
    "    if src_full and os.path.exists(src_full):\n",
    "        shutil.copy2(src_full, dst_full)\n",
    "        copy_ok += 1\n",
    "    else:\n",
    "        print(f\"[Sublabel ì—†ìŒ] {isbn}\")\n",
    "        copy_fail += 1\n",
    "print(f\"Sublabel ë³µì‚¬: {copy_ok}ê°œ ì„±ê³µ, {copy_fail}ê°œ ì‹¤íŒ¨\")\n",
    "\n",
    "# --- 4. ê²°ê³¼ ì €ì¥ ---\n",
    "meta_simple_path = os.path.join(json_dir, 'book_titles_by_isbn.json')\n",
    "with open(meta_simple_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump([{\"isbn\": m[\"isbn\"], \"title\": m[\"title\"]} for m in meta_list], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "meta_all_path = os.path.join(json_dir, 'book_meta_all.json')\n",
    "with open(meta_all_path, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump(meta_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "meta_books_dir = os.path.join(json_dir, 'books_by_isbn')\n",
    "os.makedirs(meta_books_dir, exist_ok=True)\n",
    "for isbn, meta in meta_dict.items():\n",
    "    meta_path = os.path.join(meta_books_dir, f'{isbn}.json')\n",
    "    with open(meta_path, 'w', encoding='utf-8-sig') as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "removed_path = os.path.join(json_dir, 'removed_books.csv')\n",
    "pd.DataFrame([isbn_info[i] for i in sorted(isbn_removed_set)]).to_csv(removed_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"[ìµœì¢…] ì „ì²´ ê³ ìœ  isbn: {len(isbn_all_set)}\")\n",
    "print(f\"[ìµœì¢…] ëª…ì‹œì  ì§ˆë¬¸ ìˆëŠ” ì±…: {len(isbn_explicit_set)}\")\n",
    "print(f\"[ìµœì¢…] ëª…ì‹œì  ì§ˆë¬¸ ì—†ëŠ” ì±…: {len(isbn_removed_set)}\")\n",
    "print(f\"ëª¨ë“  ì „ì²˜ë¦¬/ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d08bf351-2ade-4468-bb72-b8eb751f5191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ì „ì²´ JSON íŒŒì¼ ìˆ˜: 1721\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ JSON íŒŒì¼ ê°œìˆ˜ ì„¸ê¸°\n",
    "json_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"./converted/Sublabel\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            json_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"ğŸ“‚ ì „ì²´ JSON íŒŒì¼ ìˆ˜: {len(json_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64284079-a4f2-4654-8228-ab5907f2ee8d",
   "metadata": {},
   "source": [
    "### Unpacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f878010-1550-4ebd-a5a4-14e06ce399ce",
   "metadata": {},
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "base_dir = './converted/sublabel'\n",
    "\n",
    "# ì´ë™ëœ íŒŒì¼ ìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•œ ë³€ìˆ˜\n",
    "moved_files_count = 0\n",
    "\n",
    "# 01.ì›ì²œë°ì´í„° í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "if not os.path.isdir(base_dir):\n",
    "    print(f\"ì˜¤ë¥˜: ì§€ì •ëœ ê²½ë¡œ '{base_dir}'ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ í´ë”ê°€ ì•„ë‹™ë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"'{base_dir}' í´ë”ì—ì„œ .json íŒŒì¼ ì´ë™ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir, topdown=False):\n",
    "        if root == base_dir:\n",
    "            continue\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.json'):\n",
    "                source_path = os.path.join(root, file_name)\n",
    "                destination_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "                # íŒŒì¼ëª… ì¶©ëŒ ë°©ì§€: ì´ë¯¸ ëŒ€ìƒ í´ë”ì— ë™ì¼í•œ íŒŒì¼ëª…ì´ ì¡´ì¬í•  ê²½ìš° ì²˜ë¦¬\n",
    "                if os.path.exists(destination_path):\n",
    "                    # ì¶©ëŒ í•´ê²° ì „ëµ: (3) ì´ë¦„ ë³€ê²½\n",
    "                    base, ext = os.path.splitext(file_name)\n",
    "                    counter = 1\n",
    "                    new_file_name = f\"{base}_{counter}{ext}\"\n",
    "                    while os.path.exists(os.path.join(base_dir, new_file_name)):\n",
    "                        counter += 1\n",
    "                        new_file_name = f\"{base}_{counter}{ext}\"\n",
    "                    \n",
    "                    destination_path = os.path.join(base_dir, new_file_name)\n",
    "                    print(f\"íŒŒì¼ëª… ì¶©ëŒ: '{file_name}' -> '{new_file_name}'ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì´ë™í•©ë‹ˆë‹¤.\")\n",
    "                \n",
    "                try:\n",
    "                    shutil.move(source_path, destination_path)\n",
    "                    print(f\"'{source_path}' -> '{destination_path}' (ì´ë™ ì™„ë£Œ)\")\n",
    "                    moved_files_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"'{source_path}' ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        \n",
    "        # íŒŒì¼ ì´ë™ í›„, í˜„ì¬ í•˜ìœ„ ë””ë ‰í† ë¦¬(root)ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‚­ì œ \n",
    "        try:\n",
    "            if not os.listdir(root):\n",
    "                os.rmdir(root)\n",
    "                print(f\"ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ: '{root}'\")\n",
    "        except OSError as e:\n",
    "            print(f\"ë¹ˆ ë””ë ‰í† ë¦¬ '{root}' ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì•„ë§ˆë„ ë¹„ì–´ìˆì§€ ì•Šê±°ë‚˜ ê¶Œí•œ ë¬¸ì œ): {e}\")\n",
    "\n",
    "    print(f\"\\nëª¨ë“  .json íŒŒì¼ ì´ë™ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {moved_files_count}ê°œì˜ íŒŒì¼ì´ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342b87d-cdfb-48e4-883c-d1e7bd0786e9",
   "metadata": {},
   "source": [
    "### Lookup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76a6661a-6295-4618-9aff-247f4777e5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./converted\\training' ì—ì„œ .json íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘ ì¤‘...\n",
      "'./converted\\training' ì—ì„œ ì´ 1443ê°œì˜ ê³ ìœ  .json íŒŒì¼ëª… ìˆ˜ì§‘ ì™„ë£Œ.\n",
      "'./converted\\validation' ì—ì„œ .json íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘ ì¤‘...\n",
      "'./converted\\validation' ì—ì„œ ì¶”ê°€ë¡œ 284ê°œì˜ ê³ ìœ  .json íŒŒì¼ëª… ìˆ˜ì§‘ ì™„ë£Œ.\n",
      "\n",
      "ì´ (training + validation) ê³ ìœ  .json íŒŒì¼ ê°œìˆ˜: 1727ê°œ\n",
      "\n",
      "'./converted\\sublabel' ì—ì„œ .json íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘ ì¤‘...\n",
      "'./converted\\sublabel' ì—ì„œ ì´ 2342ê°œì˜ ê³ ìœ  .json íŒŒì¼ëª… ìˆ˜ì§‘ ì™„ë£Œ.\n",
      "\n",
      "--- ë¹„êµ ê²°ê³¼ ---\n",
      "Training/Validation ì´ ê³ ìœ  íŒŒì¼ ê°œìˆ˜: 1727ê°œ\n",
      "Sublabel ì´ ê³ ìœ  íŒŒì¼ ê°œìˆ˜: 2342ê°œ\n",
      "\n",
      "âŒ Sublabelì— ì—†ëŠ” Training/Validation íŒŒì¼ ê°œìˆ˜: 4ê°œ\n",
      "\n",
      "Sublabelì— ì—†ëŠ” íŒŒì¼ ëª©ë¡:\n",
      "- 03_02T_02S_9788961914314.json\n",
      "- 03_03T_03S_9791128208904.json\n",
      "- 03_03T_03S_9791159420054_.json\n",
      "- 03_03T_03S_9791159420160_.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_sublabel_completeness(converted_base_dir):\n",
    "    \"\"\"\n",
    "    training, validation ê²½ë¡œì˜ .json íŒŒì¼ëª…ë“¤ì„ sublabel ê²½ë¡œì˜ .json íŒŒì¼ëª…ê³¼ ë¹„êµí•˜ì—¬\n",
    "    ëˆ„ë½ëœ ëª©ë¡ê³¼ ê°œìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        converted_base_dir (str): 'converted' í´ë”ì˜ ì ˆëŒ€ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ.\n",
    "                                  ì˜ˆ: './converted' ë˜ëŠ” 'C:/Users/user/project/converted'\n",
    "    \"\"\"\n",
    "    training_dir = os.path.join(converted_base_dir, 'training')\n",
    "    validation_dir = os.path.join(converted_base_dir, 'validation')\n",
    "    sublabel_dir = os.path.join(converted_base_dir, 'sublabel')\n",
    "\n",
    "    # 1. training ë° validation ê²½ë¡œì˜ ëª¨ë“  JSON íŒŒì¼ëª… ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)\n",
    "    all_source_filenames = set()\n",
    "\n",
    "    # training ë””ë ‰í† ë¦¬ íƒìƒ‰\n",
    "    print(f\"'{training_dir}' ì—ì„œ .json íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    if not os.path.isdir(training_dir):\n",
    "        print(f\"ê²½ê³ : '{training_dir}' ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        for root, _, files in os.walk(training_dir):\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.json'):\n",
    "                    all_source_filenames.add(file_name)\n",
    "        print(f\"'{training_dir}' ì—ì„œ ì´ {len(all_source_filenames)}ê°œì˜ ê³ ìœ  .json íŒŒì¼ëª… ìˆ˜ì§‘ ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "    # validation ë””ë ‰í† ë¦¬ íƒìƒ‰ (ê¸°ì¡´ all_source_filenamesì— ì¶”ê°€)\n",
    "    print(f\"'{validation_dir}' ì—ì„œ .json íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    if not os.path.isdir(validation_dir):\n",
    "        print(f\"ê²½ê³ : '{validation_dir}' ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        current_source_count = len(all_source_filenames)\n",
    "        for root, _, files in os.walk(validation_dir):\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.json'):\n",
    "                    all_source_filenames.add(file_name)\n",
    "        print(f\"'{validation_dir}' ì—ì„œ ì¶”ê°€ë¡œ {len(all_source_filenames) - current_source_count}ê°œì˜ ê³ ìœ  .json íŒŒì¼ëª… ìˆ˜ì§‘ ì™„ë£Œ.\")\n",
    "\n",
    "    total_source_count = len(all_source_filenames)\n",
    "    print(f\"\\nì´ (training + validation) ê³ ìœ  .json íŒŒì¼ ê°œìˆ˜: {total_source_count}ê°œ\")\n",
    "\n",
    "    # 2. sublabel ê²½ë¡œì˜ ëª¨ë“  JSON íŒŒì¼ëª… ìˆ˜ì§‘\n",
    "    all_sublabel_filenames = set()\n",
    "    print(f\"\\n'{sublabel_dir}' ì—ì„œ .json íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    if not os.path.isdir(sublabel_dir):\n",
    "        print(f\"ì˜¤ë¥˜: '{sublabel_dir}' ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ í´ë”ê°€ ì—†ìœ¼ë©´ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return # sublabel í´ë”ê°€ ì—†ìœ¼ë©´ í•¨ìˆ˜ ì¢…ë£Œ\n",
    "\n",
    "    for root, _, files in os.walk(sublabel_dir):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.json'):\n",
    "                all_sublabel_filenames.add(file_name)\n",
    "\n",
    "    total_sublabel_count = len(all_sublabel_filenames)\n",
    "    print(f\"'{sublabel_dir}' ì—ì„œ ì´ {total_sublabel_count}ê°œì˜ ê³ ìœ  .json íŒŒì¼ëª… ìˆ˜ì§‘ ì™„ë£Œ.\")\n",
    "\n",
    "    # 3. ë¹„êµ ë° ê²°ê³¼ ì¶œë ¥\n",
    "    # training/validationì—ëŠ” ìˆì§€ë§Œ sublabelì—ëŠ” ì—†ëŠ” íŒŒì¼\n",
    "    missing_in_sublabel = all_source_filenames - all_sublabel_filenames\n",
    "    \n",
    "    print(\"\\n--- ë¹„êµ ê²°ê³¼ ---\")\n",
    "    print(f\"Training/Validation ì´ ê³ ìœ  íŒŒì¼ ê°œìˆ˜: {total_source_count}ê°œ\")\n",
    "    print(f\"Sublabel ì´ ê³ ìœ  íŒŒì¼ ê°œìˆ˜: {total_sublabel_count}ê°œ\")\n",
    "\n",
    "    if not missing_in_sublabel:\n",
    "        print(\"\\nâœ… ëª¨ë“  Training/Validation íŒŒì¼ì´ Sublabelì— ì¡´ì¬í•©ë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Sublabelì— ì—†ëŠ” Training/Validation íŒŒì¼ ê°œìˆ˜: {len(missing_in_sublabel)}ê°œ\")\n",
    "        print(\"\\nSublabelì— ì—†ëŠ” íŒŒì¼ ëª©ë¡:\")\n",
    "        # ì •ë ¬í•˜ì—¬ ì¶œë ¥í•˜ë©´ ë³´ê¸° ì¢‹ìŠµë‹ˆë‹¤.\n",
    "        for filename in sorted(list(missing_in_sublabel)):\n",
    "            print(f\"- {filename}\")\n",
    "\n",
    "# --- í•¨ìˆ˜ í˜¸ì¶œ ---\n",
    "# ì—¬ê¸°ì— 'converted' í´ë”ì˜ ì‹¤ì œ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
    "# ì˜ˆ: converted_base_directory = 'C:/Users/ì‚¬ìš©ìëª…/ë‚´í”„ë¡œì íŠ¸/converted'\n",
    "# ë˜ëŠ” í˜„ì¬ ì£¼í”¼í„° ë…¸íŠ¸ë¶ íŒŒì¼ì´ 'converted' í´ë”ì™€ ê°™ì€ ë ˆë²¨ì— ìˆë‹¤ë©´:\n",
    "converted_base_directory = './converted' \n",
    "\n",
    "check_sublabel_completeness(converted_base_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e9dde-5060-4b67-8ec3-ff3bb3b2ae1c",
   "metadata": {},
   "source": [
    "### Check token size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ead606-bb94-42a4-8d28-cb5383039353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sys, re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE = Path(\"./converted/formatted\")\n",
    "SPLITS = [\"train\", \"val\"]\n",
    "\n",
    "def load_json(fp):\n",
    "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def fail(msg, errors, fp):\n",
    "    errors.append(f\"{fp} :: {msg}\")\n",
    "\n",
    "def validate_file(fp):\n",
    "    errors=[]\n",
    "    try:\n",
    "        js = load_json(fp)\n",
    "    except Exception as e:\n",
    "        return [f\"{fp} :: JSON load error â€“ {e}\"]\n",
    "\n",
    "    if \"data\"   not in js: fail(\"'data' key missing\", errors, fp)\n",
    "    if \"version\" not in js: fail(\"'version' key missing\", errors, fp)\n",
    "\n",
    "    for d_i, data in enumerate(js.get(\"data\", [])):\n",
    "        if \"title\" not in data: fail(f\"[data[{d_i}]] 'title' missing\", errors, fp)\n",
    "        for p_i, para in enumerate(data.get(\"paragraphs\", [])):\n",
    "            context = para.get(\"context\")\n",
    "            if context is None: fail(f\"[{d_i}][{p_i}] context missing\", errors, fp)\n",
    "            for q_i, qa in enumerate(para.get(\"qas\", [])):\n",
    "                qid = qa.get(\"id\", \"<no-id>\")\n",
    "                if \"question\" not in qa:  fail(f\"{qid} question missing\", errors, fp)\n",
    "                if \"is_impossible\" not in qa:\n",
    "                    fail(f\"{qid} is_impossible missing\", errors, fp)\n",
    "                    continue\n",
    "                imps = qa[\"is_impossible\"]\n",
    "\n",
    "                ans_list = qa.get(\"answers\", [])\n",
    "                if imps and ans_list:\n",
    "                    fail(f\"{qid} marked impossible but answers provided\", errors, fp)\n",
    "                if not imps and not ans_list:\n",
    "                    fail(f\"{qid} possible but answers empty\", errors, fp)\n",
    "\n",
    "                # answer-context ì •í•©ì„±\n",
    "                for a_i, ans in enumerate(ans_list):\n",
    "                    text = ans.get(\"text\")\n",
    "                    pos  = ans.get(\"answer_start\")\n",
    "                    if text is None or pos is None:\n",
    "                        fail(f\"{qid} answer[{a_i}] missing field\", errors, fp)\n",
    "                        continue\n",
    "                    if context is not None and context[pos:pos+len(text)] != text:\n",
    "                        snippet = context[pos:pos+len(text)]\n",
    "                        fail(f\"{qid} answer mismatch (ctx:'{snippet}' vs ans:'{text}')\", errors, fp)\n",
    "    return errors\n",
    "\n",
    "def validate_split(split):\n",
    "    print(f\"\\nğŸ” VALIDATE {split.upper()}\")\n",
    "    split_dir = BASE / split\n",
    "    issues=[]\n",
    "    for fp in tqdm(list(split_dir.glob(\"*.json\"))):\n",
    "        issues.extend(validate_file(fp))\n",
    "    if issues:\n",
    "        print(f\"âŒ {len(issues)} issue(s) found in {split} files\")\n",
    "        for msg in issues[:20]:   # ì²˜ìŒ 20ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°\n",
    "            print(\"   â€¢\", msg)\n",
    "    else:\n",
    "        print(\"âœ… all {split} files are valid\")\n",
    "\n",
    "for s in SPLITS:\n",
    "    validate_split(s)\n",
    "\n",
    "print(\"\\nğŸŸ¢ ê²€ì¦ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ff4f6-64db-4ace-b631-c07fe18eb6a2",
   "metadata": {},
   "source": [
    "### formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bfc5a6a-fc6b-49a0-bd7f-5f2b83f4cb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„  loading â€¦\n",
      "  train label 1443 | val label 284 | sub 1725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8feee4a60fe42ee86149ccdb5c91962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAIN:   0%|          | 0/1443 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… train.json â€” books: 1429  skipped_isbn: 1\n",
      "ğŸ”¹ ì´ QA ìˆ˜: 2521 | ë¯¸í¬í•¨ QA ìˆ˜: 85\n",
      "   â†ªï¸  ë§ë­‰ì¹˜ ëˆ„ë½ ISBN ì˜ˆì‹œ: ['9788961914314']\n",
      "   â†ªï¸  ì •ë‹µ ë¯¸í¬í•¨ QA ì˜ˆì‹œ:\n",
      "      â€¢ [9791128212765] Q: ë¬´ì„œìš´ ë±€ì¸ì¤„ ì•Œê³  ì–´ë–»ê²Œ í•˜ìê³  í–ˆë‚˜ìš”? / A: ë„ë§ê°€ì.\n",
      "      â€¢ [9791128216725] Q: ì—¬ìš°ê°€ ìš°ì£¼ì„ ì„ íƒ€ê³  ìˆëŠ” ê±¸ ë³¸ ì¹œêµ¬ë“¤ì€ ë­ë¼ê³  í–ˆë‚˜ìš”? / A: ìš°ë¦¬ë„ íƒœì›Œ ì¤˜.\n",
      "      â€¢ [9791128216985] Q: ë„í† ë¦¬ ë‹¬ì„ ë¨¹ì€ ë‹¤ëŒì¥ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”? / A: ë‹¤ëŒì¥ê°€ ì›ƒì–´ìš”. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf793afca4042be9afd455a7af49c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VAL:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… val.json â€” books: 276  skipped_isbn: 1\n",
      "ğŸ”¹ ì´ QA ìˆ˜: 512 | ë¯¸í¬í•¨ QA ìˆ˜: 22\n",
      "   â†ªï¸  ë§ë­‰ì¹˜ ëˆ„ë½ ISBN ì˜ˆì‹œ: ['9791128208904']\n",
      "   â†ªï¸  ì •ë‹µ ë¯¸í¬í•¨ QA ì˜ˆì‹œ:\n",
      "      â€¢ [9791165432423] Q: ì¹œêµ¬ì™€ í•¨ê»˜ ê·¸ë„¤ë¥¼ íƒ”ì„ ë•Œ ì–´ë–¤ ê¸°ë¶„ì„ ëŠë‚„ê¹Œìš”? / A: ì‹ ë‚œë‹¤.\n",
      "      â€¢ [9791165434724] Q: ì•„ì´ì˜ ë°°ë³€ì„ ë„ì™€ì£¼ë ¤ê³  í•˜ëŠ” ì´ëŠ” ëˆ„êµ¬ì¸ê°€ìš”? / A: ì•¼ì˜¹ì´\n",
      "      â€¢ [9791165435950] Q: ì±…ì€ í˜¼ì ë³´ëŠ” ê²ƒë³´ë‹¤ ì–´ë–»ê²Œ ë³´ëŠ” ê²ƒì´ ë” ì¬ë¯¸ìˆì„ê¹Œìš”? / A: ì¹œêµ¬ì™€ í•¨ê»˜ ë³´ë©´ ë” ì¬ë¯¸ë‚˜ìš”.\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸŸ¢  Cell 1   â•‘  KorQuAD í˜•ì‹ ë§ë­‰ì¹˜ ìƒì„± ìŠ¤í¬ë¦½íŠ¸\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import json, re, hashlib\n",
    "from pathlib import Path\n",
    "from typing import Union, Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT      = Path('./converted')\n",
    "TRAIN_LB  = ROOT/'training'  /'02.ë¼ë²¨ë§ë°ì´í„°'\n",
    "VAL_LB    = ROOT/'validation'/'02.ë¼ë²¨ë§ë°ì´í„°'\n",
    "SUB_DIR   = ROOT/'sublabel'\n",
    "OUT_DIR   = ROOT/'formatted'; OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ISBN ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ISBN_RE = re.compile(r'(\\d{9,13}X?)_?(?=\\.json$)')\n",
    "def isbn_from_name(src: Union[str, Path]) -> str:\n",
    "    name = src.name if hasattr(src, 'name') else str(src)\n",
    "    m = ISBN_RE.search(name)\n",
    "    if not m:\n",
    "        raise ValueError(f'ISBN not found: {name}')\n",
    "    return m.group(1)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_dir(p: Path) -> Dict[str, dict]:\n",
    "    return {isbn_from_name(f): json.loads(f.read_text('utf-8-sig')) for f in p.glob('*.json')}\n",
    "\n",
    "def load_sublabels(p: Path) -> Dict[str, str]:\n",
    "    tmp: Dict[str, List[str]] = {}\n",
    "    for f in p.glob('*.json'):\n",
    "        isbn = isbn_from_name(f)\n",
    "        txt = json.loads(f.read_text('utf-8-sig'))['text']\n",
    "        tmp.setdefault(isbn, []).append(txt)\n",
    "    return {k: '\\n\\n'.join(v) for k, v in tmp.items()}\n",
    "\n",
    "print(\"ğŸ“„  loading â€¦\")\n",
    "train_lbl = load_dir(TRAIN_LB)\n",
    "val_lbl   = load_dir(VAL_LB)\n",
    "sub_txt   = load_sublabels(SUB_DIR)\n",
    "print(f\"  train label {len(train_lbl)} | val label {len(val_lbl)} | sub {len(sub_txt)}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ KorQuAD entry ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def entry(isbn: str, lbl: dict, context: str) -> tuple[dict, list]:\n",
    "    qas, missed = [], []\n",
    "    for qa in lbl[\"paragraphInfo\"][0][\"queAnsPairInfo\"]:\n",
    "        qid = f'{isbn}-{hashlib.md5(qa[\"question\"].encode()).hexdigest()[:8]}'\n",
    "        ans = qa[\"ansM1\"]\n",
    "        if not ans: continue\n",
    "        start = context.find(ans)\n",
    "        if start == -1:\n",
    "            missed.append((qa[\"question\"], ans))\n",
    "            continue\n",
    "        qas.append({\n",
    "            \"id\": qid,\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"answers\": [{\"text\": ans, \"answer_start\": start}]\n",
    "        })\n",
    "    return {\n",
    "        \"title\": lbl[\"title\"],\n",
    "        \"paragraphs\": [{\n",
    "            \"context\": context,\n",
    "            \"qas\": qas\n",
    "        }]\n",
    "    }, missed\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Split ìƒì„± ë° ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_split(lbl_pool: Dict[str, dict], split: str):\n",
    "    data, skip_isbn = [], []\n",
    "    total_qas, missed_qas_total = 0, 0\n",
    "    missed_detail = []\n",
    "\n",
    "    for isbn, lbl in tqdm(lbl_pool.items(), desc=f\"{split.upper()}\"):\n",
    "        context = sub_txt.get(isbn)\n",
    "        if not context:\n",
    "            skip_isbn.append(isbn)\n",
    "            continue\n",
    "\n",
    "        record, missed = entry(isbn, lbl, context)\n",
    "        total_qas += sum(1 for _ in lbl[\"paragraphInfo\"][0][\"queAnsPairInfo\"])\n",
    "        missed_qas_total += len(missed)\n",
    "        if record[\"paragraphs\"][0][\"qas\"]:\n",
    "            data.append(record)\n",
    "        if missed:\n",
    "            missed_detail.append((isbn, missed))\n",
    "\n",
    "    out_path = OUT_DIR / f\"{split}.json\"\n",
    "    json.dump({\"version\": \"v1.0\", \"data\": data}, out_path.open('w', encoding='utf-8'), ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… {out_path.name} â€” books: {len(data)}  skipped_isbn: {len(skip_isbn)}\")\n",
    "    print(f\"ğŸ”¹ ì´ QA ìˆ˜: {total_qas} | ë¯¸í¬í•¨ QA ìˆ˜: {missed_qas_total}\")\n",
    "    if skip_isbn:\n",
    "        print(f\"   â†ªï¸  ë§ë­‰ì¹˜ ëˆ„ë½ ISBN ì˜ˆì‹œ: {skip_isbn[:5]}\")\n",
    "    if missed_detail:\n",
    "        print(\"   â†ªï¸  ì •ë‹µ ë¯¸í¬í•¨ QA ì˜ˆì‹œ:\")\n",
    "        for isbn, qa_list in missed_detail[:3]:\n",
    "            for q, a in qa_list[:2]:\n",
    "                print(f\"      â€¢ [{isbn}] Q: {q} / A: {a}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "make_split(train_lbl, 'train')\n",
    "make_split(val_lbl,   'val')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c43529-b2a5-4063-880b-121a8eab4078",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547a1c76-d9e9-4953-b1f3-b114ed5915cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“œ train.json\n",
      "  articles        : 1429\n",
      "  paragraphs      : 1429\n",
      "  QAs             : 2436\n",
      "  â”œâ”€â”€ empty answer      : 0\n",
      "  â”œâ”€â”€ start OOB         : 0\n",
      "  â””â”€â”€ substring mismatch: 0\n",
      "  context length  : max 185806, avg 3079.5\n",
      "  answer length   : max 63, avg 5.5\n",
      "\n",
      "ğŸ“œ val.json\n",
      "  articles        : 276\n",
      "  paragraphs      : 276\n",
      "  QAs             : 490\n",
      "  â”œâ”€â”€ empty answer      : 0\n",
      "  â”œâ”€â”€ start OOB         : 0\n",
      "  â””â”€â”€ substring mismatch: 0\n",
      "  context length  : max 129815, avg 2454.8\n",
      "  answer length   : max 36, avg 5.5\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸŸ¢  Cell 2   â•‘  ìƒì„¸ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import json, textwrap, statistics\n",
    "from pathlib import Path\n",
    "\n",
    "def validate(split: str, root: Path = Path(\"./converted/formatted\")):\n",
    "    path = root / f\"{split}.json\"\n",
    "    if not path.exists():\n",
    "        print(f\"{split}: íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    blob = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    art_cnt = par_cnt = qa_cnt = 0\n",
    "    empty_ans = out_of_bounds = mismatch = 0\n",
    "    bad_samples = []\n",
    "\n",
    "    ctx_lengths = []\n",
    "    ans_lengths = []\n",
    "\n",
    "    for art in blob[\"data\"]:\n",
    "        art_cnt += 1\n",
    "        for par in art[\"paragraphs\"]:\n",
    "            par_cnt += 1\n",
    "            ctx = par[\"context\"]\n",
    "            ctx_lengths.append(len(ctx))\n",
    "            for qa in par[\"qas\"]:\n",
    "                qa_cnt += 1\n",
    "                if not qa[\"answers\"]:\n",
    "                    empty_ans += 1\n",
    "                    bad_samples.append((\"EMPTY\", qa[\"id\"], qa[\"question\"][:40]))\n",
    "                    continue\n",
    "                ans = qa[\"answers\"][0]\n",
    "                text, start = ans[\"text\"], ans[\"answer_start\"]\n",
    "                ans_lengths.append(len(text))\n",
    "\n",
    "                if start < 0 or start + len(text) > len(ctx):\n",
    "                    out_of_bounds += 1\n",
    "                    bad_samples.append((\"OOB\", qa[\"id\"], qa[\"question\"][:40]))\n",
    "                elif ctx[start:start + len(text)] != text:\n",
    "                    mismatch += 1\n",
    "                    bad_samples.append((\"MISMATCH\", qa[\"id\"], qa[\"question\"][:40]))\n",
    "\n",
    "    print(f\"\\nğŸ“œ {path.name}\")\n",
    "    print(f\"  articles        : {art_cnt}\")\n",
    "    print(f\"  paragraphs      : {par_cnt}\")\n",
    "    print(f\"  QAs             : {qa_cnt}\")\n",
    "    print(f\"  â”œâ”€â”€ empty answer      : {empty_ans}\")\n",
    "    print(f\"  â”œâ”€â”€ start OOB         : {out_of_bounds}\")\n",
    "    print(f\"  â””â”€â”€ substring mismatch: {mismatch}\")\n",
    "\n",
    "    if ctx_lengths:\n",
    "        print(f\"  context length  : max {max(ctx_lengths)}, \"\n",
    "              f\"avg {statistics.mean(ctx_lengths):.1f}\")\n",
    "    if ans_lengths:\n",
    "        print(f\"  answer length   : max {max(ans_lengths)}, \"\n",
    "              f\"avg {statistics.mean(ans_lengths):.1f}\")\n",
    "\n",
    "    if bad_samples:\n",
    "        print(\"\\n  âš ï¸  ì²« 5ê°œ ì˜¤ë¥˜ ìƒ˜í”Œ\")\n",
    "        for kind, qid, qpreview in bad_samples[:5]:\n",
    "            print(f\"   [{kind}] {qid}  |  {qpreview}â€¦\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "validate(\"train\")\n",
    "validate(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832f81a-0122-498f-91c4-f53a1159b444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff09b1-407b-45d1-ac23-d031429bebed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a192461-79c9-44ce-8a74-13e7b2b7cedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
