{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcd972b-a0d4-42fe-859d-ba22ff38fa0f",
   "metadata": {},
   "source": [
    "### ëª¨ë“ˆ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2dff119-409d-4af0-8eba-211310e12918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json, sys, re\n",
    "import shutil\n",
    "import collections\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import hashlib, itertools\n",
    "import csv   \n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing   import Union, Dict, List, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e67c9-925e-49cb-937c-52e406bfbf88",
   "metadata": {},
   "source": [
    "### unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4874a149-2e43-4ad2-aca0-857ce948bc4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ğŸš€] ./Origin/Sublabel í•˜ìœ„ì˜ ëª¨ë“  .jsonì„ ./converted/Sublabelë¡œ í‰íƒ„í™”(flatten)\n",
      "[âœ… ì™„ë£Œ] ./converted/Sublabel ì— 2345ê°œ ë³µì‚¬ ì™„ë£Œ\n",
      "\n",
      "[ğŸš€] ./Origin/Training/02.ë¼ë²¨ë§ë°ì´í„° í•˜ìœ„ì˜ ëª¨ë“  .jsonì„ ./converted/Trainingë¡œ í‰íƒ„í™”(flatten)\n",
      "[âœ… ì™„ë£Œ] ./converted/Training ì— 1446ê°œ ë³µì‚¬ ì™„ë£Œ\n",
      "\n",
      "[ğŸš€] ./Origin/Validation/02.ë¼ë²¨ë§ë°ì´í„° í•˜ìœ„ì˜ ëª¨ë“  .jsonì„ ./converted/Validationë¡œ í‰íƒ„í™”(flatten)\n",
      "[âœ… ì™„ë£Œ] ./converted/Validation ì— 286ê°œ ë³µì‚¬ ì™„ë£Œ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "    # (source, target)\n",
    "    ('./Origin/Sublabel',                 './converted/Sublabel'),\n",
    "    ('./Origin/Training/02.ë¼ë²¨ë§ë°ì´í„°',   './converted/Training'),\n",
    "    ('./Origin/Validation/02.ë¼ë²¨ë§ë°ì´í„°', './converted/Validation'),\n",
    "]\n",
    "\n",
    "for source_dir, target_dir in tasks:\n",
    "    copied_files = 0\n",
    "\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"[âŒ ì˜¤ë¥˜] ê²½ë¡œ '{source_dir}'ê°€ ì—†ìŒ\")\n",
    "        continue\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(f\"[ğŸš€] {source_dir} í•˜ìœ„ì˜ ëª¨ë“  .jsonì„ {target_dir}ë¡œ í‰íƒ„í™”(flatten)\")\n",
    "\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if root == target_dir:\n",
    "            continue\n",
    "        for file_name in files:\n",
    "            if not file_name.endswith('.json'):\n",
    "                continue\n",
    "\n",
    "            src = os.path.join(root, file_name)\n",
    "            dst = os.path.join(target_dir, file_name)\n",
    "\n",
    "            # â”€â”€ íŒŒì¼ëª… ì¶©ëŒ ëŒ€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(file_name)\n",
    "                i = 1\n",
    "                while os.path.exists(os.path.join(target_dir, f\"{base}_{i}{ext}\")):\n",
    "                    i += 1\n",
    "                new_name = f\"{base}_{i}{ext}\"\n",
    "                dst = os.path.join(target_dir, new_name)\n",
    "                print(f\"âš ï¸ íŒŒì¼ëª… ì¶©ëŒ: '{file_name}' â†’ '{new_name}' ë¡œ ë³µì‚¬\")\n",
    "\n",
    "            try:\n",
    "                shutil.copy2(src, dst)    \n",
    "                copied_files += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[ì˜¤ë¥˜] '{src}' ë³µì‚¬ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    print(f\"[âœ… ì™„ë£Œ] {target_dir} ì— {copied_files}ê°œ ë³µì‚¬ ì™„ë£Œ\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf8eec-9394-4148-b8ce-09e98158a539",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ì…‹ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753a328d-34db-4e8e-a368-b1efa7a6b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ í´ë” ê¸°ì¤€ í›ˆë ¨ ë°ì´í„° ê³ ìœ ê¶Œìˆ˜ : 1446\n"
     ]
    }
   ],
   "source": [
    "isbns = set()\n",
    "for dirpath, _, filenames in os.walk('./Origin/Training'):\n",
    "    for fn in filenames:\n",
    "        if fn.lower().endswith('.json'):\n",
    "            with open(os.path.join(dirpath, fn), encoding='utf-8-sig') as f:\n",
    "                d = json.load(f)\n",
    "            books = d if isinstance(d, list) else [d]\n",
    "            for book in books:\n",
    "                isbns.add(str(book.get('isbn','')).strip())\n",
    "print(f\"ì›ë³¸ í´ë” ê¸°ì¤€ í›ˆë ¨ ë°ì´í„° ê³ ìœ ê¶Œìˆ˜ : {len(isbns)}\")  # ì›ë³¸ í´ë” ê¸°ì¤€ ê³ ìœ ê¶Œìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61647f68-687d-4b15-8f49-0ab2233ce7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ í´ë” ê¸°ì¤€ ê²€ì¦ ë°ì´í„° ê³ ìœ ê¶Œìˆ˜ : 286\n"
     ]
    }
   ],
   "source": [
    "isbns = set()\n",
    "for dirpath, _, filenames in os.walk('./Origin/Validation'):\n",
    "    for fn in filenames:\n",
    "        if fn.lower().endswith('.json'):\n",
    "            with open(os.path.join(dirpath, fn), encoding='utf-8-sig') as f:\n",
    "                d = json.load(f)\n",
    "            books = d if isinstance(d, list) else [d]\n",
    "            for book in books:\n",
    "                isbns.add(str(book.get('isbn','')).strip())\n",
    "print(f\"ì›ë³¸ í´ë” ê¸°ì¤€ ê²€ì¦ ë°ì´í„° ê³ ìœ ê¶Œìˆ˜ : {len(isbns)}\")  # ì›ë³¸ í´ë” ê¸°ì¤€ ê³ ìœ ê¶Œìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae55056-b635-44a0-9bc2-0314a8a8f90f",
   "metadata": {},
   "source": [
    "### ëª…ì‹œì  ì§ˆë¬¸ë§Œ ë‚¨ê¸°ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6bbb07-4d42-41c2-8b6e-b01c662ad3fb",
   "metadata": {},
   "source": [
    "â˜… ë¼ë²¨ë§ ë°ì´í„°ë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcb36b3-84a7-4f66-9d89-d2bb7816a8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70418f51d427472aaec80a368317912c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAINING:   0%|          | 0/1446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“œ training.json\n",
      "  articles   :  1442\n",
      "  paragraphs :  1442\n",
      "  QAs        : 47443\n",
      "    â””â”€ ë§¤ì¹­ ì‹¤íŒ¨ : 1159\n",
      "  â”€â”€ mismatch preview â”€â”€\n",
      "   â€¢ [9791128216893] ã…‚ìœ¼ë¡œ ì‹œì‘ë˜ëŠ” ì´ë¦„ì„ ê°€ì§„ ê³¼ì¼ ì¤‘ ê¸¸ì­‰í•œ ê²ƒì€ ë¬´ì—‡ì¼ê¹Œìš”?  â†’  ë°”ë‚˜ë‚˜\n",
      "   â€¢ [9791128216893] ì²« ê¸€ìë¥¼ ë³´ê³  ìš°ë¦¬ëŠ” ë¬´ì—‡ì„ í•´ì•¼ í•˜ë‚˜ìš”?  â†’  ì²« ê¸€ìë¡œ ì‹œì‘í•˜ëŠ” ì´ë¦„ì„ ì°¾ì•„ì•¼ í•´ìš”.\n",
      "   â€¢ [9791159424502] ì•„ì´ë“¤ì€ ì •ì›ì—ì„œ ì–´ë–»ê²Œ ë†€ì•˜ë‚˜ìš”?  â†’  ì‹ ë‚˜ê²Œ ë†€ì•˜ì–´ìš”.\n",
      "   â€¢ [9791159424564] ê¼¬íˆ¬ë¦¬ë¥¼ êº¾ì—ˆì„ ë•Œ ì™„ë‘ì½© ì˜¤í˜•ì œëŠ” ë°”ê¹¥ì„¸ìƒìœ¼ë¡œ ë‚˜ê°ˆ ìƒê°ì— ì–´ë–»ê²Œ í–ˆë‚˜ìš”?  â†’  ì™„ë‘ì½© ì˜¤í˜•ì œëŠ” ê°€ìŠ´ì´ ë‘ê·¼ê±°ë ¸ì–´ìš”.\n",
      "   â€¢ [9791159424632] ë¹¨ê°„ ëª¨ìëŠ” ì–´ë””ë¥¼ ê±¸ì–´ê°”ë‚˜ìš”?  â†’  ìˆ²ì†\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc593e017f44193909a1749107b2182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VALIDATION:   0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“œ validation.json\n",
      "  articles   :   282\n",
      "  paragraphs :   282\n",
      "  QAs        :  5998\n",
      "    â””â”€ ë§¤ì¹­ ì‹¤íŒ¨ : 174\n",
      "  â”€â”€ mismatch preview â”€â”€\n",
      "   â€¢ [9791128216954] ë¹—ë°©ìš¸ì´ ì°½ë¬¸ì„ ë‘ë“œë¦° ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?  â†’  ì§‘ì—ë§Œ ìˆì§€ë§ê³  ë‚˜ì™€ì„œ ë†€ìê³ \n",
      "   â€¢ [9791159424700] ì´ìƒí•œ ì¼ì€ ì–´ë””ì—ì„œ ì¼ì–´ë‚¬ë‚˜ìš”?  â†’  í• ë¨¸ë‹ˆ ì§‘\n",
      "   â€¢ [9791165434724] ì•„ì´ì˜ ë°°ë³€ì„ ë„ì™€ì£¼ë ¤ê³  í•˜ëŠ” ì´ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?  â†’  ì•¼ì˜¹ì´\n",
      "   â€¢ [9791165435950] ì±…ì€ í˜¼ì ë³´ëŠ” ê²ƒë³´ë‹¤ ì–´ë–»ê²Œ ë³´ëŠ” ê²ƒì´ ë” ì¬ë¯¸ìˆì„ê¹Œìš”?  â†’  ì¹œêµ¬ì™€ í•¨ê»˜ ë³´ë©´ ë” ì¬ë¯¸ë‚˜ìš”.\n",
      "   â€¢ [9791165436025] ì—„ë§ˆ ë°°ì— ì†ì„ ëŒ€ì–´ ë³¸ í† ë¦¬ëŠ” ì•„ê¸°ê°€ ë­˜ í•œë‹¤ê³  ìƒê°í–ˆë‚˜ìš”?  â†’  ê¹¡ì´ê¹¡ì´ ë›°ë‚˜ ë´ìš”.\n",
      "\n",
      "ğŸŸ¢ All done! â†’ cleaned/, formatted/  ë””ë ‰í„°ë¦¬ í™•ì¸\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘   KorQuAD v1 ë§ë­‰ì¹˜ + cleaned ë³¸ë¬¸   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²½ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT = Path(\"./converted\") ; ROOT.mkdir(exist_ok=True)     \n",
    "CLEAN_DIR = ROOT / \"cleaned\"       ; CLEAN_DIR.mkdir(exist_ok=True)\n",
    "FMT_DIR   = ROOT / \"formatted\"     ; FMT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "LBL_DIRS  = {\"training\": ROOT / \"training\",\n",
    "             \"validation\": ROOT / \"validation\"}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ISBN ì¶”ì¶œ util â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ISBN_RE = re.compile(r\"([0-9X]{9,13})(?=_?\\.json$)\", re.I)\n",
    "def isbn_from(path: Union[str, Path]) -> str:\n",
    "    path = Path(path)\n",
    "    m = ISBN_RE.search(path.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"ISBN not found in filename: {path.name}\")\n",
    "    return m.group(1)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì •ê·œí™” & back-map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PUNCT = re.compile(r\"[^\\w\\s]\")      # í•„ìš”í•˜ë©´ ë²”ìœ„ ì¡°ì •\n",
    "def normalize_with_map(text: str) -> Tuple[str, List[int]]:\n",
    "    norm_chars, back = [], []\n",
    "    for i, ch in enumerate(text):\n",
    "        if PUNCT.match(ch):           # ë¬¸ì¥ë¶€í˜¸ skip\n",
    "            continue\n",
    "        norm_chars.append(ch)\n",
    "        back.append(i)\n",
    "    return \"\".join(norm_chars), back\n",
    "\n",
    "def locate_answer(ctx: str, ans: str) -> Tuple[int, str] | Tuple[None, None]:\n",
    "    \"\"\"ì›ë¬¸ ctx ì—ì„œ ans ìœ„ì¹˜ ë°˜í™˜. ì‹¤íŒ¨í•˜ë©´ punctuation-agnostic ì¬íƒìƒ‰.\"\"\"\n",
    "    pos = ctx.find(ans)\n",
    "    if pos != -1:\n",
    "        return pos, ans\n",
    "    # â€• ì¬íƒìƒ‰ â€•\n",
    "    ctx_norm, back = normalize_with_map(ctx)\n",
    "    ans_norm, _    = normalize_with_map(ans)\n",
    "    p2 = ctx_norm.find(ans_norm)\n",
    "    if p2 == -1:\n",
    "        return None, None\n",
    "    start = back[p2]\n",
    "    end   = back[p2 + len(ans_norm) - 1]\n",
    "    return start, ctx[start:end+1]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Split ì²˜ë¦¬ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def process_split(split: str, src_root: Path) -> None:\n",
    "    korquad_data   : List[dict]      = []\n",
    "    total_qas = missed_qas = 0\n",
    "    missed_log : list[tuple] = []    # (isbn, Q, A)\n",
    "\n",
    "    for lbl_file in tqdm(list(src_root.glob(\"*.json\")), desc=split.upper()):\n",
    "        isbn = isbn_from(lbl_file)\n",
    "        book  = json.loads(lbl_file.read_text(encoding=\"utf-8-sig\"))\n",
    "\n",
    "        # â‘   context  â”€ í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ëª¨ì•„ ë¶™ì´ê¸°\n",
    "        paras = sorted(book.get(\"paragraphInfo\", []),\n",
    "                       key=lambda p: p.get(\"srcPage\", 0))\n",
    "        context = \"\\n\\n\".join(p.get(\"srcText\", \"\") for p in paras).strip()\n",
    "\n",
    "        # cleaned/<isbn>.json : í•œ ë²ˆë§Œ ì €ì¥\n",
    "        cleaned_path = CLEAN_DIR / f\"{isbn}.json\"\n",
    "        if not cleaned_path.exists():\n",
    "            cleaned_path.write_text(\n",
    "                json.dumps({\"isbn\": isbn,\n",
    "                            \"title\": book.get(\"title\", \"\"),\n",
    "                            \"text\": context},\n",
    "                           ensure_ascii=False, indent=2),\n",
    "                encoding=\"utf-8-sig\"\n",
    "            )\n",
    "\n",
    "        # â‘¡  KorQuAD QA ë³€í™˜  (ëª…ì‹œì  ì§ˆë¬¸ë§Œ)\n",
    "        qas: List[dict] = []\n",
    "        for qa in itertools.chain.from_iterable(\n",
    "                p.get(\"queAnsPairInfo\", []) for p in paras):\n",
    "            if \"ëª…ì‹œì \" not in qa.get(\"ansType\", \"\"):\n",
    "                continue\n",
    "            total_qas += 1\n",
    "            qid = f\"{isbn}-{hashlib.md5(qa['question'].encode()).hexdigest()[:8]}\"\n",
    "            ans_raw = qa.get(\"ansM1\", \"\").strip()\n",
    "            if not ans_raw:\n",
    "                missed_qas += 1\n",
    "                missed_log.append((isbn, qa[\"question\"], \"<EMPTY>\"))\n",
    "                continue\n",
    "            start, span = locate_answer(context, ans_raw)\n",
    "            if start is None:\n",
    "                missed_qas += 1\n",
    "                missed_log.append((isbn, qa[\"question\"], ans_raw))\n",
    "                continue\n",
    "            qas.append({\n",
    "                \"id\": qid,\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answers\": [{\"text\": span, \"answer_start\": start}]\n",
    "            })\n",
    "\n",
    "        if qas:          # QA ê°€ í•˜ë‚˜ë¼ë„ ìˆì–´ì•¼ í¬í•¨\n",
    "            korquad_data.append({\n",
    "                \"title\": book.get(\"title\", isbn),\n",
    "                \"paragraphs\": [{\n",
    "                    \"context\": context,\n",
    "                    \"qas\": qas\n",
    "                }]\n",
    "            })\n",
    "\n",
    "    # â‘¢  KorQuAD íŒŒì¼ ì €ì¥\n",
    "    out_file = FMT_DIR / f\"{split}.json\"\n",
    "    out_file.write_text(\n",
    "        json.dumps({\"version\": \"v1.0\", \"data\": korquad_data},\n",
    "                   ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    # â‘£  ë¦¬í¬íŠ¸\n",
    "    print(f\"\\nğŸ“œ {out_file.name}\")\n",
    "    print(f\"  articles   : {len(korquad_data):5d}\")\n",
    "    print(f\"  paragraphs : {len(korquad_data):5d}\")\n",
    "    print(f\"  QAs        : {total_qas:5d}\")\n",
    "    print(f\"    â””â”€ ë§¤ì¹­ ì‹¤íŒ¨ : {missed_qas}\")\n",
    "    if missed_log:\n",
    "        print(\"  â”€â”€ mismatch preview â”€â”€\")\n",
    "        for isbn, q, a in missed_log[:5]:\n",
    "            print(f\"   â€¢ [{isbn}] {q}  â†’  {a}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    for split, root in LBL_DIRS.items():\n",
    "        process_split(split, root)\n",
    "\n",
    "    print(\"\\nğŸŸ¢ All done! â†’ cleaned/, formatted/  ë””ë ‰í„°ë¦¬ í™•ì¸\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ff4f6-64db-4ace-b631-c07fe18eb6a2",
   "metadata": {},
   "source": [
    "### formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bfc5a6a-fc6b-49a0-bd7f-5f2b83f4cb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a936397c01d34aebb6352b87ae537dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAINING:   0%|          | 0/1446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… training.json   paragraphs:  27662\n",
      "   total QA:  80043 | missed: 33615\n",
      "   â†ªï¸  miss example:\n",
      "      â€¢ ISBN 9788961915915 p4 Q:ì–´ëŠ ë‚  í• ë¨¸ë‹ˆê°€ ê½ƒë°­ì— ë¬¼ì„ ì£¼ê³  ìˆëŠ”ë° ë¬´ìŠ¨ ì¼ì´ ë²Œì–´ì¡Œë‚˜ìš”?  A:ê°‘ìê¸° í•˜ëŠ˜ì—ì„œ ì»¤ë‹¤ë€ ìš°ë°•ì´ ë–¨ì–´ì¡Œì–´ìš”.\n",
      "      â€¢ ISBN 9788961915953 p4 Q:ì‹ ì˜ê°€ ì‹ ì´ ë‚˜ì„œ í˜ê» ë‹¬ë ¤ê°„ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?  A:ì•„ë¹ ì™€ ëª¨ì²˜ëŸ¼ ë‚˜ë“¤ì´ë¥¼ ë‚˜ì™”ê¸° ë•Œë¬¸ì´ì—ìš”.\n",
      "      â€¢ ISBN 9788961916080 p5 Q:ê°™ì€ ê³µë£¡ì„ ì¡ì•„ë¨¹ëŠ” ìœ¡ì‹ ê³µë£¡ì€ ì–´ë•Œ ë³´ì—¬ìš”?  A:ë¬´ì‹œë¬´ì‹œí•´ ë³´ì—¬ìš”.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a79af157824666b80b2f50508ac189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VALIDATION:   0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… validation.json  paragraphs:   3249\n",
      "   total QA:  10005 | missed:  4179\n",
      "   â†ªï¸  miss example:\n",
      "      â€¢ ISBN 9788961916028 p4 Q:ë‚´ê°€ ì£¼ëŠ” íŒíŠ¸ë¥¼ ë“£ê³  ë¬´ì—‡ì„ í•´ ë³¼ ê²ƒì´ëƒê³  ë¬¼ì–´ë´¤ë‚˜ìš”?  A:ë‚´ê°€ ëˆ„êµ°ì§€ ë§í˜€ ë³¼ ê²ƒì´ëƒê³  ë¬¼ì–´ë´¤ì–´ìš”.\n",
      "      â€¢ ISBN 9788961916103 p3 Q:ë¦´ë¦¬ì—”íƒˆê³¼ êµ¬ìŠ¤íƒ€í”„ëŠ” í•˜ëŠ˜ì˜ ìƒˆë¥¼ êµ¬ê²½í•˜ê¸° ìœ„í•´ ë§¤ì¼ ë¬´ì—‡ì„ í–ˆë‚˜ìš”?  A:ì–¸ë•ì— ì˜¬ëì–´ìš”.\n",
      "      â€¢ ISBN 9788961916103 p3 Q:í˜•ì œëŠ” ììœ ë¡­ê²Œ í•˜ëŠ˜ì„ ë‚˜ëŠ” ìƒˆë¥¼ ë³´ë©° ì–´ë–¤ ê°ì •ì´ ë“¤ì—ˆë‚˜ìš”?  A:ìƒˆë¥¼ ë¶€ëŸ¬ì›Œí–ˆì–´ìš”.\n",
      "\n",
      "ğŸŸ¢  Done â€” check formatted/miss_*.csv for full mismatch list.\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸŸ¢  Cell 1   â•‘  KorQuAD ìƒì„± + ë§¤ì¹­ ì‹¤íŒ¨ ë””ë²„ê·¸ ë¡œê·¸ â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ROOT          = Path(\"./converted\")\n",
    "TRAIN_LB      = ROOT / \"training\"\n",
    "VAL_LB        = ROOT / \"validation\"\n",
    "CLEAN_DIR     = ROOT / \"cleaned\"     \n",
    "OUT_DIR       = ROOT / \"formatted\"   \n",
    "\n",
    "ISBN_RE = re.compile(r\"(\\d{9,13}X?)_?(?=\\.json$)\")\n",
    "def isbn_from_name(p: Union[str, Path]) -> str:\n",
    "    m = ISBN_RE.search(Path(p).name)\n",
    "    if not m: raise ValueError(f\"ISBN not found in {p}\")\n",
    "    return m.group(1)\n",
    "\n",
    "def load_pool(dir_: Path) -> Dict[str, dict]:\n",
    "    return {isbn_from_name(f): json.loads(f.read_text(\"utf-8-sig\"))\n",
    "            for f in dir_.glob(\"*.json\")}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ cleaned íŒŒì¼ ë³µì‚¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def save_clean(isbn: str, book: dict):\n",
    "    pages = sorted(book[\"paragraphInfo\"], key=lambda x: x.get(\"srcPage\", 0))\n",
    "    (CLEAN_DIR / f\"{isbn}.json\").write_text(\n",
    "        json.dumps(\n",
    "            {\"isbn\": isbn,\n",
    "             \"title\": book.get(\"title\", \"\"),\n",
    "             \"pages\":[{\"page\":p.get(\"srcPage\"),\"text\":p[\"srcText\"]} for p in pages]},\n",
    "            ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í˜ì´ì§€â†’KorQuAD + miss ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def para_to_korquad(isbn: str, title: str, para: dict\n",
    ") -> Tuple[dict, List[dict]]:\n",
    "    ctx = para[\"srcText\"]\n",
    "    page = para.get(\"srcPage\")\n",
    "    qas, misses = [], []\n",
    "    for qa in para.get(\"queAnsPairInfo\", []):\n",
    "        ans = qa.get(\"ansM1\")\n",
    "        if not ans:\n",
    "            continue\n",
    "        start = ctx.find(ans)\n",
    "        if start == -1:\n",
    "            near = ctx[:300] if len(ctx) < 300 else ctx[max(0,start-15):start+len(ans)+15]\n",
    "            misses.append(\n",
    "                {\"isbn\":isbn, \"page\":page, \"question\":qa[\"question\"], \"answer\":ans,\n",
    "                 \"ctx_slice\":near.replace(\"\\n\",\" \"), \"reason\":\"not-found\"}\n",
    "            )\n",
    "            continue\n",
    "        qid = f\"{isbn}-{hashlib.md5(qa['question'].encode()).hexdigest()[:8]}\"\n",
    "        qas.append({\"id\":qid,\"question\":qa[\"question\"],\n",
    "                    \"answers\":[{\"text\":ans,\"answer_start\":start}]})\n",
    "    return ({\"title\":title,\"paragraphs\":[{\"context\":ctx,\"qas\":qas}]}, misses)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ split ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def split_to_json(pool: Dict[str, dict], split_name: str):\n",
    "    korquad, miss_log = [], []\n",
    "    total_q, miss_q = 0, 0\n",
    "\n",
    "    for isbn, book in tqdm(pool.items(), desc=split_name.upper()):\n",
    "        save_clean(isbn, book)\n",
    "\n",
    "        for para in sorted(book[\"paragraphInfo\"],\n",
    "                           key=lambda x: x.get(\"srcPage\",0)):\n",
    "            total_q += len(para.get(\"queAnsPairInfo\",[]))\n",
    "            rec, miss = para_to_korquad(isbn, book.get(\"title\",\"\"), para)\n",
    "            miss_q   += len(miss)\n",
    "            miss_log.extend(miss)\n",
    "            if rec[\"paragraphs\"][0][\"qas\"]:\n",
    "                korquad.append(rec)\n",
    "\n",
    "    # KorQuAD json\n",
    "    out_json = OUT_DIR / f\"{split_name}.json\"\n",
    "    json.dump({\"version\":\"v1.0\",\"data\":korquad},\n",
    "              out_json.open(\"w\",encoding=\"utf-8\"),\n",
    "              ensure_ascii=False, indent=2)\n",
    "\n",
    "    # miss CSV\n",
    "    if miss_log:\n",
    "        miss_csv = OUT_DIR / f\"miss_{split_name}.csv\"\n",
    "        with miss_csv.open(\"w\",newline=\"\",encoding=\"utf-8-sig\") as f:\n",
    "            writer = csv.DictWriter(\n",
    "                f, fieldnames=[\"split\",\"isbn\",\"page\",\"question\",\"answer\",\"ctx_slice\",\"reason\"]\n",
    "            )\n",
    "            writer.writeheader()\n",
    "            for row in miss_log:\n",
    "                row[\"split\"]=split_name\n",
    "                writer.writerow(row)\n",
    "\n",
    "    # ìš”ì•½ ì¶œë ¥\n",
    "    print(f\"\\nâœ… {out_json.name:14}  paragraphs:{len(korquad):7d}\")\n",
    "    print(f\"   total QA: {total_q:6d} | missed: {miss_q:5d}\")\n",
    "    if miss_log:\n",
    "        print(\"   â†ªï¸  miss example:\")\n",
    "        for ex in miss_log[:3]:\n",
    "            print(f\"      â€¢ ISBN {ex['isbn']} p{ex['page']} Q:{ex['question']}  A:{ex['answer']}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_pool, val_pool = load_pool(TRAIN_LB), load_pool(VAL_LB)\n",
    "split_to_json(train_pool, \"training\")\n",
    "split_to_json(val_pool,   \"validation\")\n",
    "print(\"\\nğŸŸ¢  Done â€” check formatted/miss_*.csv for full mismatch list.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c43529-b2a5-4063-880b-121a8eab4078",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547a1c76-d9e9-4953-b1f3-b114ed5915cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” training.json\n",
      "  QA total : 46,428\n",
      "  âœ… all offsets OK\n",
      "\n",
      "ğŸ” validation.json\n",
      "  QA total : 5,826\n",
      "  âœ… all offsets OK\n",
      "\n",
      "ğŸ“‚ cleaned dir : 1,732 files\n",
      "   KorQuAD ISBN referenced : 0\n",
      "   missing cleaned files   : 0 (none)\n",
      "\n",
      "ğŸŸ¢  verification done\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸŸ¢  Cell 2   â•‘  KorQuAD / cleaned ê²€ì¦    â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import json, csv, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT       = Path(\"./converted\")\n",
    "FMT_DIR    = ROOT / \"formatted\"\n",
    "CLEAN_DIR  = ROOT / \"cleaned\"\n",
    "\n",
    "SPLITS = {\"training\":\"training.json\", \"validation\":\"validation.json\"}\n",
    "\n",
    "def verify_korquad(split:str, file:Path)-> List[Dict]:\n",
    "    \"\"\"answer_start ì •í•©ì„± & ë²”ìœ„ ì²´í¬ â†’ ì‹¤íŒ¨ row ëª©ë¡ ë°˜í™˜\"\"\"\n",
    "    bad : List[Dict] = []\n",
    "    data = json.loads(file.read_text(encoding=\"utf-8\"))\n",
    "    for art in data[\"data\"]:\n",
    "        title  = art.get(\"title\",\"\")\n",
    "        for para in art[\"paragraphs\"]:\n",
    "            ctx    = para[\"context\"]\n",
    "            ctxlen = len(ctx)\n",
    "            for qa in para[\"qas\"]:\n",
    "                for ans in qa[\"answers\"]:\n",
    "                    text  = ans[\"text\"]\n",
    "                    start = ans[\"answer_start\"]\n",
    "                    # â‘  ë‹µì´ contextì— ì¡´ì¬?\n",
    "                    found_at = ctx.find(text)\n",
    "                    # â‘¡ start ë²”ìœ„, slice ì¼ì¹˜\n",
    "                    slice_ok = 0 <= start <= ctxlen-len(text) and ctx[start:start+len(text)]==text\n",
    "                    if slice_ok and found_at==start:\n",
    "                        continue   # ì •ìƒ\n",
    "                    # --- ë¬¸ì œ ê¸°ë¡\n",
    "                    reason  = (\"not-found\"      if found_at==-1 else\n",
    "                               \"misalign\"      if found_at!=start else\n",
    "                               \"slice-mismatch\")\n",
    "                    bad.append({\n",
    "                        \"split\":split,\n",
    "                        \"title\":title,\n",
    "                        \"id\":qa[\"id\"],\n",
    "                        \"question\":qa[\"question\"],\n",
    "                        \"answer\":text,\n",
    "                        \"start\":start,\n",
    "                        \"found_at\":found_at,\n",
    "                        \"reason\":reason,\n",
    "                        \"ctx_slice\":ctx[max(0,start-20):min(ctxlen,start+len(text)+20)].replace(\"\\n\",\" \")\n",
    "                    })\n",
    "    return bad\n",
    "\n",
    "def save_csv(rows:List[Dict], out:Path):\n",
    "    if not rows: return\n",
    "    with out.open(\"w\",newline=\"\",encoding=\"utf-8-sig\") as f:\n",
    "        w=csv.DictWriter(f,fieldnames=list(rows[0]))\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "\n",
    "# â”€â”€â”€â”€â”€ 1) KorQuAD íŒŒì¼ ê²€ì‚¬ â”€â”€â”€â”€â”€\n",
    "isbn_in_kqa=set()\n",
    "for split,fname in SPLITS.items():\n",
    "    fpath = FMT_DIR / fname\n",
    "    if not fpath.exists(): \n",
    "        print(f\"âš ï¸  {fname} not found\"); continue\n",
    "    bad = verify_korquad(split,fpath)\n",
    "    save_csv(bad, FMT_DIR/f\"bad_{split}.csv\")\n",
    "    # í†µê³„\n",
    "    total   = sum( len(p[\"qas\"]) for a in json.loads(fpath.read_text(encoding=\"utf-8\"))[\"data\"]\n",
    "                                for p in a[\"paragraphs\"])\n",
    "    print(f\"\\nğŸ” {fname}\")\n",
    "    print(f\"  QA total : {total:,}\")\n",
    "    print(f\"  bad rows : {len(bad):,}   â‡¢  saved to bad_{split}.csv\" if bad else \"  âœ… all offsets OK\")\n",
    "    if bad:                        # í™”ë©´ ì˜ˆì‹œ\n",
    "        for row in bad[:3]:\n",
    "            print(f\"   â€¢ {row['id']} | reason:{row['reason']} \"\n",
    "                  f\" start {row['start']} / found {row['found_at']} : â€œ{row['answer']}â€\")\n",
    "\n",
    "    # ISBN ì¶”ì \n",
    "    for art in json.loads(fpath.read_text(encoding=\"utf-8\"))[\"data\"]:\n",
    "        tit = art.get(\"title\",\"\")\n",
    "        m = re.fullmatch(r\"\\d{9,13}X?\", tit)\n",
    "        if m: isbn_in_kqa.add(m.group())\n",
    "\n",
    "# â”€â”€â”€â”€â”€ 2) cleaned íŒŒì¼ ì¡´ì¬ ì²´í¬ â”€â”€â”€â”€â”€\n",
    "missing = [ {\"isbn\":isbn} for isbn in sorted(isbn_in_kqa)\n",
    "            if not (CLEAN_DIR/f\"{isbn}.json\").exists() ]\n",
    "save_csv(missing, FMT_DIR/\"missing_cleaned.csv\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ cleaned dir : {len(list(CLEAN_DIR.glob('*.json'))):,} files\")\n",
    "print(f\"   KorQuAD ISBN referenced : {len(isbn_in_kqa):,}\")\n",
    "print(f\"   missing cleaned files   : {len(missing):,}\"\n",
    "      + (f\"  â‡¢ saved to missing_cleaned.csv\" if missing else \" (none)\"))\n",
    "\n",
    "print(\"\\nğŸŸ¢  verification done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8d1ab-2a7d-4fee-84c6-7931ad5b2eb1",
   "metadata": {},
   "source": [
    "### Metatdata ìƒì„± (for web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "434a99aa-8dec-4ded-8403-1a7c16a06d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾  meta\\book_meta_all_training.json  (1446 books)\n",
      "ğŸ’¾  meta\\book_meta_all_validation.json  (286 books)\n",
      "ğŸŸ¢  meta files created\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸŸ¢  Cell 3 â€“ meta íŒŒì¼ 2ì¢…   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ROOT      = Path(\"./converted\")\n",
    "TRAIN_LB  = ROOT / \"training\"\n",
    "VAL_LB    = ROOT / \"validation\"\n",
    "META_DIR   = ROOT / \"meta\"; META_DIR.mkdir(exist_ok=True)\n",
    "ISBN_RE = re.compile(r\"([0-9X]{9,13})(?=_?\\.json$)\")\n",
    "\n",
    "def isbn_from_name(p:Path)->str:\n",
    "    m = ISBN_RE.search(p.name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def load_pool(p_dir:Path):\n",
    "    return {isbn_from_name(f):json.loads(f.read_text(\"utf-8-sig\"))\n",
    "            for f in p_dir.glob(\"*.json\")}\n",
    "\n",
    "def to_meta_dict(isbn:str, book:dict):\n",
    "    meta = {\n",
    "        \"isbn\"         : isbn,\n",
    "        \"title\"        : book.get(\"title\",\"\").strip(),\n",
    "        \"author\"       : book.get(\"author\"),\n",
    "        \"illustrator\"  : book.get(\"illustrator\"),\n",
    "        \"publisher\"    : book.get(\"publisher\"),\n",
    "        \"publishedYear\": book.get(\"publishedYear\"),\n",
    "        \"readAge\"      : book.get(\"readAge\"),\n",
    "        \"classification\": book.get(\"classification\"),\n",
    "    }\n",
    "    # ë¹ˆ ê°’ì€ ì œê±°\n",
    "    return {k:v for k,v in meta.items() if v}\n",
    "\n",
    "def build_meta(pool:dict, split:str):\n",
    "    metas = [to_meta_dict(i,bk) for i,bk in pool.items()]\n",
    "    out   = META_DIR / f\"book_meta_all_{split}.json\"\n",
    "    out.write_text(json.dumps(metas, ensure_ascii=False, indent=2),\n",
    "                   encoding=\"utf-8\")\n",
    "    print(f\"ğŸ’¾  {out.relative_to(ROOT)}  ({len(metas)} books)\")\n",
    "\n",
    "train_pool = load_pool(TRAIN_LB)\n",
    "val_pool   = load_pool(VAL_LB)\n",
    "\n",
    "build_meta(train_pool, \"training\")\n",
    "build_meta(val_pool,   \"validation\")\n",
    "print(\"ğŸŸ¢  meta files created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c364babf-088a-4441-96a9-939ba0aae91a",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4579b868-55b8-4e75-a704-13c66f787cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… converted\\meta\\book_meta_all.json ì €ì¥ ì™„ë£Œ (1732 ê¶Œ)\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ê²½ë¡œ\n",
    "TRAIN_META = Path('./converted/meta/book_meta_all_training.json')\n",
    "VAL_META   = Path('./converted/meta/book_meta_all_validation.json')\n",
    "OUT_META   = Path('./converted/meta/book_meta_all.json')\n",
    "\n",
    "# ë¡œë“œ\n",
    "with TRAIN_META.open(encoding='utf-8-sig') as f:\n",
    "    train_meta = json.load(f)\n",
    "with VAL_META.open(encoding='utf-8-sig') as f:\n",
    "    val_meta = json.load(f)\n",
    "\n",
    "# ISBN ì¤‘ë³µ ì œê±° (training ìš°ì„ , ì—†ìœ¼ë©´ validation)\n",
    "meta_dict = {item['isbn']: item for item in val_meta}\n",
    "meta_dict.update({item['isbn']: item for item in train_meta})  # trainì´ ìš°ì„ \n",
    "\n",
    "# ì €ì¥\n",
    "with OUT_META.open('w', encoding='utf-8-sig') as f:\n",
    "    json.dump(list(meta_dict.values()), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… {OUT_META} ì €ì¥ ì™„ë£Œ ({len(meta_dict)} ê¶Œ)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e670d6-c7ac-492b-8e92-a4f9c3bb909e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175db0f-ffb7-452f-bdc8-9dac120ddce7",
   "metadata": {},
   "source": [
    "# ë !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19968f2-10fc-4a65-9252-a44e4caa5780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
