{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10PNS_iPUkQ-",
        "outputId": "5ff1b77a-bead-46ae-b855-55a87a7cf1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import"
      ],
      "metadata": {
        "id": "WbzmNkyujZIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    BertTokenizerFast, BertForQuestionAnswering,\n",
        "    TrainingArguments, Trainer\n",
        ")"
      ],
      "metadata": {
        "id": "mbOs_7wZjYcn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Í≤ΩÎ°ú ÏßÄÏ†ï"
      ],
      "metadata": {
        "id": "JkVBvbEzjbVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/NLP (1)/training.json\"\n",
        "val_path   = \"/content/drive/MyDrive/NLP (1)/validation.json\"\n",
        "model_ckpt = \"beomi/kcbert-base\""
      ],
      "metadata": {
        "id": "CPkNkyJ2jcSX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
      ],
      "metadata": {
        "id": "vwlYyRCZjfFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(model_ckpt)\n",
        "tokenizer.model_max_length = 300            # ‚Üê 300 Ïù¥ÌïòÎ°ú Í≥†Ï†ï\n",
        "MAX_LEN   = 256                             # Ïã§Ï†ú ÏûÖÎ†• Í∏∏Ïù¥\n",
        "DOC_STRIDE = 128                            # Ïä¨ÎùºÏù¥Îî© ÏúàÎèÑ\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained(model_ckpt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GcL6K20jg0-",
        "outputId": "94fb61bc-3c0d-4a4d-ea23-c37ddd07ce41"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞"
      ],
      "metadata": {
        "id": "85X40DhMjiDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(json_path):\n",
        "    with open(json_path, encoding=\"utf-8\") as f:\n",
        "        raw = json.load(f)[\"data\"]\n",
        "    buf = []\n",
        "    for art in raw:\n",
        "        for para in art[\"paragraphs\"]:\n",
        "            ctx = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                for ans in qa[\"answers\"]:\n",
        "                    buf.append({\n",
        "                        \"id\"     : qa[\"id\"],\n",
        "                        \"context\": ctx,\n",
        "                        \"question\": qa[\"question\"],\n",
        "                        \"answer_text\" : ans[\"text\"],\n",
        "                        \"answer_start\": ans[\"answer_start\"],\n",
        "                    })\n",
        "    return buf\n",
        "\n",
        "train_samples = flatten(train_path)\n",
        "val_samples   = flatten(val_path)\n",
        "\n",
        "\n",
        "class KorQuAD(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.samples[idx]\n",
        "\n",
        "        enc = tokenizer(\n",
        "            ex[\"question\"], ex[\"context\"],\n",
        "            max_length=MAX_LEN,\n",
        "            truncation=\"only_second\",\n",
        "            stride=DOC_STRIDE,\n",
        "            return_overflowing_tokens=False,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        offset = enc.pop(\"offset_mapping\")[0]\n",
        "        ans_s, ans_e = ex[\"answer_start\"], ex[\"answer_start\"] + len(ex[\"answer_text\"])\n",
        "\n",
        "        tok_start = tok_end = None\n",
        "        for i, (s, e) in enumerate(offset):\n",
        "            if s <= ans_s < e: tok_start = i\n",
        "            if s <  ans_e <= e: tok_end   = i\n",
        "        # ÎãµÏù¥ ÏûòÎ†§ ÎÇòÍ∞ÄÎ©¥ ÏÉòÌîå drop\n",
        "        if tok_start is None or tok_end is None:\n",
        "            return self.__getitem__((idx+1)%len(self))   # Ïû¨Í∑ÄÎ°ú Îã§Ïùå ÏÉòÌîå\n",
        "\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"start_positions\"] = torch.tensor(tok_start)\n",
        "        item[\"end_positions\"]   = torch.tensor(tok_end)\n",
        "        return item\n",
        "\n",
        "train_ds = KorQuAD(train_samples)\n",
        "val_ds   = KorQuAD(val_samples)"
      ],
      "metadata": {
        "id": "bOjcC4p2jlNS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ÌõàÎ†®ÌïòÍ∏∞"
      ],
      "metadata": {
        "id": "PcYBp6t6jl0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "YOFyva11PZO4",
        "outputId": "f58b3f0c-2600-4ef4-b29a-f54161836ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-2187846205>:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29020' max='29020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29020/29020 1:15:59, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.804200</td>\n",
              "      <td>0.794534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.519600</td>\n",
              "      <td>0.786869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.330600</td>\n",
              "      <td>0.984986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.182900</td>\n",
              "      <td>1.456881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.072800</td>\n",
              "      <td>1.734986</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=29020, training_loss=0.4284141697775159, metrics={'train_runtime': 4560.6064, 'train_samples_per_second': 50.901, 'train_steps_per_second': 6.363, 'total_flos': 3.032871455434752e+16, 'train_loss': 0.4284141697775159, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir       = \"/content/qa-out\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy    = \"epoch\",\n",
        "    learning_rate    = 5e-5,\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size  = 8,\n",
        "    num_train_epochs = 5,\n",
        "    weight_decay     = 0.01,\n",
        "    report_to        = \"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model         = model,\n",
        "    args          = training_args,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset  = val_ds,\n",
        "    tokenizer     = tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ÌèâÍ∞ÄÌïòÍ∏∞"
      ],
      "metadata": {
        "id": "EbN4vv4Cqmxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Í∞ÄÏû• Í≤∞Í≥ºÍ∞Ä Ï¢ãÏïòÎçò Epoch 2 Î™®Îìà Î∂àÎü¨Ïò§Í∏∞"
      ],
      "metadata": {
        "id": "eabCSfGR68X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "checkpoint_path = \"/content/qa-out/checkpoint-11608\"\n",
        "model = BertForQuestionAnswering.from_pretrained(checkpoint_path)"
      ],
      "metadata": {
        "id": "FU2Ia6Sj6KCP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "KYYgy7XbPpwC",
        "outputId": "5c0ccc1a-0e0e-48e0-b476-456e448ebfb6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='729' max='729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [729/729 01:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä  Validation ‚îÇ Loss=1.7350 ‚îÇ EM=0.4864 ‚îÇ F1=0.6076\n"
          ]
        }
      ],
      "source": [
        "import re, string, collections, numpy as np\n",
        "from itertools import zip_longest\n",
        "\n",
        "# ‚îÄ‚îÄ 1. SQuAD Í≥µÏãù Ï†ïÍ∑úÌôî ¬∑ ÌÜ†ÌÅ∞Ìôî ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def _normalize(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = \"\".join(ch for ch in text if ch not in string.punctuation)\n",
        "    text = re.sub(r\"\\b(a|an|the)\\b\", \" \", text)        # Í¥ÄÏÇ¨ Ï†úÍ±∞\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "def _tok(text: str):\n",
        "    return _normalize(text).split()\n",
        "\n",
        "# ‚îÄ‚îÄ 2. Î©îÌä∏Î¶≠ Ìï®Ïàò (Exact-Match / F1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def exact_match(pred: str, gold: str) -> int:\n",
        "    return int(_normalize(pred) == _normalize(gold))\n",
        "\n",
        "def f1_squad(pred: str, gold: str) -> float:\n",
        "    p_toks, g_toks = map(_tok, (pred, gold))\n",
        "    common = collections.Counter(p_toks) & collections.Counter(g_toks)\n",
        "    same = sum(common.values())\n",
        "    if same == 0:\n",
        "        return 0.0\n",
        "    precision = same / len(p_toks)\n",
        "    recall    = same / len(g_toks)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "# ‚îÄ‚îÄ 3. Í≤ÄÏ¶ù ÏÖã inference ‚Üí ÏòàÏ∏° Î¨∏ÏûêÏó¥ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "start_logits, end_logits = trainer.predict(val_ds).predictions\n",
        "pred_texts = []\n",
        "for (s_log, e_log), sample in zip(zip(start_logits, end_logits), val_ds):\n",
        "    s = int(np.argmax(s_log));  e = int(np.argmax(e_log))\n",
        "    if e < s:                   e = s\n",
        "    text = tokenizer.decode(sample[\"input_ids\"][s:e+1],\n",
        "                            skip_special_tokens=True).strip()\n",
        "    pred_texts.append(text)\n",
        "\n",
        "gold_texts = [ex[\"answer_text\"] for ex in val_samples]\n",
        "\n",
        "# ‚îÄ‚îÄ 4. ÏßÄÌëú ÏßëÍ≥Ñ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "EM  = np.mean([exact_match(p, g) for p, g in zip_longest(pred_texts, gold_texts, fillvalue=\"\")])\n",
        "F1  = np.mean([f1_squad (p, g)   for p, g in zip_longest(pred_texts, gold_texts, fillvalue=\"\")])\n",
        "loss_val = trainer.evaluate(eval_dataset=val_ds).get(\"eval_loss\", float(\"nan\"))\n",
        "\n",
        "print(f\"üìä  Validation ‚îÇ Loss={loss_val:.4f} ‚îÇ EM={EM:.4f} ‚îÇ F1={F1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚îÄ‚îÄ 5. Îç∞Î™®: ÏûÑÏùò ÏÉòÌîåÎ°ú QA Ïù∏ÌÑ∞ÎûôÏÖò ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def ask_demo(idx:int=None):\n",
        "    \"\"\"idx ÏóÜÏúºÎ©¥ Í≤ÄÏ¶ùÏÖã Ï≤´ Ìï≠Î™© ÏÇ¨Ïö©\"\"\"\n",
        "    sample = val_samples[idx or 0]\n",
        "    context = sample[\"context\"]\n",
        "    print(\"‚îÄ ÏßÄÎ¨∏ ‚îÄ\")\n",
        "    print(context[:400], \"...\" if len(context)>400 else \"\")  # Í∏∏Î©¥ ÏûòÎùºÏÑú ÌëúÏãú\n",
        "    print(\"\\n(‚Äª ÏúÑ ÏßÄÎ¨∏ ÏùºÎ∂ÄÎßå ÌëúÏãú - Ï†ÑÏ≤¥Îäî model Ïóê ÏûÖÎ†•Îê©ÎãàÎã§)\\n\")\n",
        "    q = input(\"üó®Ô∏è  ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî: \").strip()\n",
        "    enc = tokenizer(q, context,\n",
        "                    truncation=\"only_second\", max_length=512,\n",
        "                    return_offsets_mapping=False, return_tensors=\"pt\").to(trainer.model.device)\n",
        "    with torch.no_grad():\n",
        "        out = trainer.model(**enc)\n",
        "    s = int(out.start_logits.argmax()); e = int(out.end_logits.argmax())\n",
        "    if e < s: e = s\n",
        "    answer = tokenizer.decode(enc[\"input_ids\"][0][s:e+1],\n",
        "                              skip_special_tokens=True).strip()\n",
        "    print(f\"ü§ñ  ÎãµÎ≥Ä: {answer}\")\n",
        "\n",
        "# ÏÇ¨Ïö© ÏòàÏãú\n",
        "ask_demo(2)     # ÏÖÄ Ïã§Ìñâ ÌõÑ ÏΩòÏÜîÏóê ÏßàÎ¨∏ ÏûÖÎ†•"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc7668rz2FBu",
        "outputId": "71556b32-7c72-44fa-8349-8749276149ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚îÄ ÏßÄÎ¨∏ ‚îÄ\n",
            "Ïñ¥Îäê ÎßàÏùÑÏóê Î¶¥Î¶¨ÏóîÌÉàÍ≥º Íµ¨Ïä§ÌÉÄÌîÑÎùºÎäî ÌòïÏ†úÍ∞Ä ÏÇ¥ÏïòÏñ¥. Î¶¥Î¶¨ÏóîÌÉàÍ≥º Íµ¨Ïä§ÌÉÄÌîÑÎäî Ïñ¥Î¶¥ ÎïåÎ∂ÄÌÑ∞ ÌïòÎäòÏùÑ ÏûêÏú†Î°≠Í≤å ÎÇ†ÏïÑÎã§ÎãàÎäî ÏÉàÎ•º Î∂ÄÎü¨ÏõåÌñàÏñ¥. Í∑∏ÎûòÏÑú ÎÇ†ÎßàÎã§ Ïñ∏ÎçïÏóê Ïò¨Îùº ÌïòÎäòÏùÑ ÎÇòÎäî ÏÉàÎ•º Íµ¨Í≤ΩÌïòÍ≥§ ÌñàÏßÄ. \n",
            "\n",
            "(‚Äª ÏúÑ ÏßÄÎ¨∏ ÏùºÎ∂ÄÎßå ÌëúÏãú - Ï†ÑÏ≤¥Îäî model Ïóê ÏûÖÎ†•Îê©ÎãàÎã§)\n",
            "\n",
            "üó®Ô∏è  ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî: Î¶¥Î¶¨ÏóîÌÉàÏùÄ Î¨¥ÏóáÏùÑ Î∂ÄÎü¨ÏõåÌñàÎÇòÏöî?\n",
            "ü§ñ  ÎãµÎ≥Ä: ÌïòÎäòÏùÑ ÏûêÏú†Î°≠Í≤å ÎÇ†ÏïÑÎã§ÎãàÎäî ÏÉà\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save"
      ],
      "metadata": {
        "id": "LeaIY-wEpr6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Î™®Îç∏Í≥º ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º ÏõêÌïòÎäî Í≤ΩÎ°úÏóê Ï†ÄÏû•\n",
        "save_path = \"/content/mymodel\""
      ],
      "metadata": {
        "id": "sXJ-iEMt4YYp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "LDg-5WL54KTG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfn1RH_appv4",
        "outputId": "17423d3c-fcc5-4ad7-a357-e5d9f5bdc11b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/mymodel/tokenizer_config.json',\n",
              " '/content/mymodel/special_tokens_map.json',\n",
              " '/content/mymodel/vocab.txt',\n",
              " '/content/mymodel/added_tokens.json',\n",
              " '/content/mymodel/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('mymodel', 'zip', './mymodel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7yqDsxpnpx0o",
        "outputId": "9ab523cc-a5e4-4d0b-de2e-de45339f7766"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/mymodel.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}